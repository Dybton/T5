{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
      "  Referenced from: /Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      " in /Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa11871e590>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "from transformers import get_linear_schedule_with_warmup, AutoConfig \n",
    "from transformers import BartTokenizer,BartModel,BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model\n",
    "from transformers import BartConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.autograd import Variable\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import socket\n",
    "from os.path import basename\n",
    "from functools import reduce\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import sys\n",
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my version of transformers is 4.15.0\n",
      "my version of pytorch is 1.10.0\n",
      "my version of pytorch_lightning is 1.9.3\n"
     ]
    }
   ],
   "source": [
    "print(\"my version of transformers is \" + transformers.__version__)\n",
    "print (\"my version of pytorch is \" + torch.__version__)\n",
    "print(\"my version of pytorch_lightning is \" + pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### States\n",
    "test_state = True\n",
    "tensorflow_active = True\n",
    "use_gpu = False\n",
    "train_state = False\n",
    "dev_mode = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToGraphQLDataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, tokenizer, type_path='train.json', block_size=102):\n",
    "        'Initialization'\n",
    "        super(TextToGraphQLDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        self.schema_ids = []\n",
    "        root_path = './SPEGQL-dataset/'\n",
    "        dataset_path = root_path + 'dataset/' + type_path\n",
    "\n",
    "        schemas_path = root_path + 'Schemas/'\n",
    "        schemas = glob.glob(schemas_path + '**/' + 'simpleSchema.json')\n",
    "\n",
    "        self.max_len = 0\n",
    "        self.name_to_schema = {}\n",
    "        for schema_path in schemas:\n",
    "            with open(schema_path, 'r', encoding='utf-8') as s:\n",
    "                data = json.load(s)\n",
    "\n",
    "                type_field_tokens = [['<t>'] + [t['name']] + ['{'] + [f['name'] for f in t['fields']] + ['}'] + ['</t>'] for t in data['types']]\n",
    "                type_field_flat_tokens = reduce(list.__add__, type_field_tokens)\n",
    "\n",
    "                arguments = [a['name'] for a in data['arguments']]\n",
    "                schema_tokens = type_field_flat_tokens + ['<a>'] + arguments + ['</a>']\n",
    "\n",
    "                path = Path(schema_path)\n",
    "                schema_name = basename(str(path.parent))\n",
    "\n",
    "                self.name_to_schema[schema_name] = schema_tokens\n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            if(dev_mode):\n",
    "                random.seed(42)\n",
    "                \n",
    "                random.shuffle(data)\n",
    "                data = data[:len(data) // 10]\n",
    "\n",
    "                # Print the first 3 data points\n",
    "                print(\"First 3 data points:\", data[:3])\n",
    "\n",
    "            print(\"Number of data points:\", len(data))\n",
    "\n",
    "            for element in data:\n",
    "                question_with_schema = 'translate English to GraphQL: ' + element['question'] + ' ' + ' '.join(self.name_to_schema[element['schemaId']])\n",
    "                tokenized_s = tokenizer.encode_plus(question_with_schema,max_length=1024, padding=True, truncation=True, return_tensors='pt')\n",
    "                self.source.append(tokenized_s)\n",
    "\n",
    "                tokenized_t = tokenizer.encode_plus(element['query'],max_length=block_size, padding='max_length', truncation=True, return_tensors='pt')\n",
    "                self.target.append(tokenized_t)\n",
    "                self.schema_ids.append(element['schemaId'])\n",
    "\n",
    "    def get_question_with_schema(self, question, schemaId):\n",
    "        return 'translate English to GraphQL: ' + question  + ' ' + ' '.join(self.name_to_schema[schemaId])\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]['input_ids'].squeeze()\n",
    "        target_ids = self.target[index]['input_ids'].squeeze()\n",
    "        src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "\n",
    "        return { \n",
    "            'source_ids': source_ids,\n",
    "            'source_mask': src_mask,\n",
    "            'target_ids': target_ids,\n",
    "            'target_ids_y': target_ids\n",
    "        }\n",
    "\n",
    "sys.modules[\"__main__\"].TextToGraphQLDataset = TextToGraphQLDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3827\n",
      "TextToGraphQLDataset test done\n"
     ]
    }
   ],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    dataset = TextToGraphQLDataset(tokenizer=tokenizer, type_path='train.json', block_size=102)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"TextToGraphQLDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGraphQLDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train.json', block_size=64):\n",
    "        'Initialization'\n",
    "        super(MaskGraphQLDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        path = './SPEGQL-dataset/dataset/' + type_path\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "          data = json.load(f)\n",
    "\n",
    "          if(dev_mode):\n",
    "            random.seed(42)\n",
    "            \n",
    "            random.shuffle(data)\n",
    "            data = data[:len(data) // 10]\n",
    "\n",
    "            # Print the first 3 data points\n",
    "            print(\"First 3 data points:\", data[:3])\n",
    "\n",
    "          print(\"Number of data points:\", len(data))\n",
    "\n",
    "          for example in data:\n",
    "\n",
    "            utterance = example['query']\n",
    "            encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "            token_count = encoded_source.shape[0]\n",
    "            repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "            for pos in range(1, token_count):\n",
    "              encoded_source = repeated_utterance[pos].clone()\n",
    "              target_id = encoded_source[pos].item()\n",
    "              if target_id == tokenizer.eos_token_id:\n",
    "                  break\n",
    "              encoded_source[pos] = tokenizer.mask_token_id\n",
    "              decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "              encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze()\n",
    "              if encoded_target is not None and torch.numel(encoded_target) > 0:\n",
    "                  self.target.append(encoded_target)\n",
    "                  self.source.append(encoded_source)\n",
    "              if torch.numel(encoded_target) > 0:\n",
    "                  self.target.append(encoded_target)\n",
    "                  self.source.append(encoded_source)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]\n",
    "        target_id = self.target[index]\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask>\n",
      "Number of data points: 3827\n",
      "MaskGraphQLDataset test done\n"
     ]
    }
   ],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = MaskGraphQLDataset(tokenizer=tokenizer, type_path='train.json', block_size=64)\n",
    "    print(\"MaskGraphQLDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskTextToGraphQLDatasetSyntheticData(Dataset):\n",
    "  def __init__(self, tokenizer, type_path='synthetic.json', block_size=64):\n",
    "      'Initialization'\n",
    "      super(MaskTextToGraphQLDatasetSyntheticData, ).__init__()\n",
    "      self.tokenizer = tokenizer\n",
    "\n",
    "      self.source = []\n",
    "      self.target = []\n",
    "      path = './SPEGQL-dataset/dataset/' + type_path\n",
    "      with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        for example in data:\n",
    "\n",
    "          utterance = example['query']\n",
    "          encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "          token_count = encoded_source.shape[0]\n",
    "          repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "          for pos in range(1, token_count):\n",
    "            encoded_source = repeated_utterance[pos].clone()\n",
    "            target_id = encoded_source[pos].item()\n",
    "            if target_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "            encoded_source[pos] = tokenizer.mask_token_id\n",
    "            decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "            encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze()\n",
    "            if encoded_target is not None and torch.numel(encoded_target) > 0:\n",
    "                self.target.append(encoded_target)\n",
    "                self.source.append(encoded_source)\n",
    "            if torch.numel(encoded_target) > 0:\n",
    "                self.target.append(encoded_target)\n",
    "                self.source.append(encoded_source)\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]\n",
    "        target_id = self.target[index]\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask>\n",
      "MaskTextToGraphQLDatasetSyntheticData test done\n"
     ]
    }
   ],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = MaskTextToGraphQLDatasetSyntheticData(tokenizer=tokenizer, type_path='synthetic.json', block_size=64)\n",
    "    print(\"MaskTextToGraphQLDatasetSyntheticData test done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154376"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.__getitem__(1)\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train_spider.json', block_size=102):\n",
    "        'Initialization'\n",
    "        super(SpiderDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        spider_path = './spider/'\n",
    "        path = spider_path + type_path\n",
    "        # TODO open up tables.json\n",
    "        # its a list of tables\n",
    "        # group by db_id \n",
    "        # grab column name from column_names_original ( each column name is a list of two. and the 2nd index {1} is the column name )\n",
    "        # grab table names from table_names (^ same as above )\n",
    "        # concat both with the english question (table names + <c> + column names + <q> english question)\n",
    "        # tokenize\n",
    "\n",
    "        # Maybe try making making more structure \n",
    "        # in the concat by using primary_keys and foreign_keys \n",
    "\n",
    "        tables_path = spider_path + 'tables.json'\n",
    "\n",
    "        with open(path, 'r') as f, open(tables_path, 'r') as t:\n",
    "          databases = json.load(t)\n",
    "          data = json.load(f)\n",
    "\n",
    "          if(dev_mode):\n",
    "            random.seed(42)\n",
    "            \n",
    "            random.shuffle(data)\n",
    "            data = data[:len(data) // 10]\n",
    "\n",
    "            # Print the first 3 data points\n",
    "            print(\"First 3 data points:\", data[:3])\n",
    "\n",
    "          print(\"Number of data points:\", len(data))\n",
    "\n",
    "          #groupby db_id \n",
    "          grouped_dbs = {}\n",
    "          for db in databases:\n",
    "            grouped_dbs[db['db_id']] = db\n",
    "          # print(grouped_dbs)\n",
    "          # end grop tables\n",
    "\n",
    "          for element in data:\n",
    "            db = grouped_dbs[element['db_id']]\n",
    "\n",
    "            # tables_names = \" \".join(db['table_names_original'])\n",
    "            db_tables = db['table_names_original']\n",
    "\n",
    "            # columns_names = \" \".join([column_name[1] for column_name in db['column_names_original'] ])\n",
    "            tables_with_columns = ''\n",
    "            for table_id, group in itertools.groupby(db['column_names_original'], lambda x: x[0]):\n",
    "              if table_id == -1:\n",
    "                continue\n",
    "\n",
    "              columns_names = \" \".join([column_name[1] for column_name in group ])\n",
    "              tables_with_columns += '<t> ' + db_tables[table_id] + ' <c> ' + columns_names + ' </c> ' + '</t> '\n",
    "\n",
    "\n",
    "            # group columns with tables. \n",
    "\n",
    "            db_with_question = 'translate English to SQL: ' + element['question'] + ' ' + tables_with_columns\n",
    "            # question_with_schema = 'translate English to GraphQL: ' + element['question']  + ' ' + ' '.join(self.name_to_schema[element['schemaId']]) + ' </s>'\n",
    "\n",
    "            tokenized_s = tokenizer.batch_encode_plus([db_with_question],max_length=1024, padding='max_length', truncation=True,return_tensors='pt')\n",
    "            # what is the largest example size?\n",
    "            # the alternative is to collate\n",
    "            #might need to collate\n",
    "            self.source.append(tokenized_s)\n",
    "\n",
    "            tokenized_t = tokenizer.batch_encode_plus([element['query']],max_length=block_size, padding='max_length', truncation=True,return_tensors='pt')\n",
    "            self.target.append(tokenized_t)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]['input_ids'].squeeze()\n",
    "        target_ids = self.target[index]['input_ids'].squeeze()\n",
    "        src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "        return { 'source_ids': source_ids,\n",
    "                'source_mask': src_mask,\n",
    "                'target_ids': target_ids,\n",
    "                'target_ids_y': target_ids}\n",
    "\n",
    "\n",
    "# # In[38]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 7000\n",
      "SpiderDataset test done\n"
     ]
    }
   ],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    dataset = SpiderDataset(tokenizer=tokenizer , type_path='train_spider.json', block_size=102)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"SpiderDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoSQLMaskDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='cosql_train.json', block_size=64):\n",
    "        'Initialization'\n",
    "        super(CoSQLMaskDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        path = './cosql_dataset/sql_state_tracking/' + type_path\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "\n",
    "          if(dev_mode):\n",
    "            random.seed(42)\n",
    "            \n",
    "            random.shuffle(data)\n",
    "            data = data[:len(data) // 10]\n",
    "\n",
    "            # Print the first 3 data points\n",
    "            print(\"First 3 data points:\", data[:3])\n",
    "\n",
    "          print(\"Number of data points:\", len(data))\n",
    "\n",
    "          for element in data:\n",
    "            for interaction in element['interaction']:\n",
    "              # repeat the squence for the amount of tokens. \n",
    "              # loop through those sequences and replace a different token in each one. \n",
    "              # the target will be that token. \n",
    "              utterance = interaction['query']\n",
    "              # tokens = utterance.split()\n",
    "              encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "              token_count = encoded_source.shape[0]\n",
    "              # print(encoded_source.shape)\n",
    "              repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "              for pos in range(1, token_count):\n",
    "                encoded_source = repeated_utterance[pos].clone()\n",
    "                target_id = encoded_source[pos].item()\n",
    "                if target_id == tokenizer.eos_token_id:\n",
    "                  break\n",
    "                # encoded_source[pos] = tokenizer.mask_token_id\n",
    "                # self.target.append(target_id)\n",
    "                # self.source.append(encoded_source)\n",
    "\n",
    "                encoded_source[pos] = tokenizer.mask_token_id\n",
    "                decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "                encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze() # should always be of size 1\n",
    "                self.target.append(encoded_target)\n",
    "                self.source.append(encoded_source)\n",
    "\n",
    "                # repeated_utterance[pos][pos] = target_token # so that the next iteration the previous token is correct\n",
    "\n",
    "                \n",
    "          \n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]#['input_ids'].squeeze()\n",
    "        target_id = self.target[index]#['input_ids'].squeeze()\n",
    "        # src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}\n",
    "                # 'source_mask': src_mask,\n",
    "                # 'target_ids': target_ids,\n",
    "                # 'target_ids_y': target_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask>\n",
      "Number of data points: 2159\n",
      "CoSQLMaskDataset test done\n"
     ]
    }
   ],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = CoSQLMaskDataset(tokenizer=tokenizer , type_path='cosql_train.json', block_size=64)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"CoSQLMaskDataset test done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5MultiSPModel(pl.LightningModule):\n",
    "  def __init__(self, hyperparams, task='denoise', test_flag='graphql', train_sampler=None, batch_size=2,temperature=1.0,top_k=50, top_p=1.0, num_beams=1 ):\n",
    "    super(T5MultiSPModel, self).__init__()\n",
    "\n",
    "    self.temperature = temperature\n",
    "    self.top_k = top_k\n",
    "    self.top_p = top_p\n",
    "    self.num_beams = num_beams\n",
    "\n",
    "    self.hyperparams = hyperparams\n",
    "\n",
    "    self.task = task\n",
    "    self.test_flag = test_flag\n",
    "    self.train_sampler = train_sampler\n",
    "    self.batch_size = batch_size\n",
    "    if self.task == 'finetune':\n",
    "      self.model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    else: \n",
    "      self.model = T5ForConditionalGeneration.from_pretrained('t5-base') # no output past? \n",
    "\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    \n",
    "    self.criterion = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    self.add_special_tokens()\n",
    "\n",
    "  def forward(\n",
    "    self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "  def add_special_tokens(self):\n",
    "    # new special tokens\n",
    "    special_tokens_dict = self.tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    self.tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "  def _step(self, batch):\n",
    "    if self.task == 'finetune':\n",
    "      pad_token_id = self.tokenizer.pad_token_id\n",
    "      source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
    "      # y_ids = y[:, :-1].contiguous()\n",
    "      labels = y[:, :].clone()\n",
    "      labels[y[:, :] == pad_token_id] = -100\n",
    "      # attention_mask is for ignore padding on source_ids \n",
    "      # labels need to have pad_token ignored manually by setting to -100\n",
    "      # todo check the ignore token for forward\n",
    "      # seems like decoder_input_ids can be removed. \n",
    "      outputs = self(source_ids, attention_mask=source_mask, labels=labels,)\n",
    "\n",
    "      loss = outputs[0]\n",
    "\n",
    "    else: \n",
    "      y = batch['target_id']\n",
    "      labels = y[:, :].clone()\n",
    "      labels[y[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "      loss = self(\n",
    "          input_ids=batch[\"source_ids\"],\n",
    "          labels=labels\n",
    "      )[0]\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    tensorboard_logs = {\"train_loss\": loss}\n",
    "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    print(f'Validation step called, batch_idx: {batch_idx}, loss: {loss.item()}')\n",
    "\n",
    "    return {\"val_loss\": loss}\n",
    "\n",
    "\n",
    "  def on_validation_epoch_end(self, outputs=None):\n",
    "    if not outputs:\n",
    "        print(\"Empty outputs list.\")\n",
    "        return\n",
    "    print(\"outputs \" + str(outputs))\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    # if self.task == 'finetune':\n",
    "    #   avg_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "    #   tensorboard_logs = {\"val_loss\": avg_loss, \"avg_val_acc\": avg_acc}\n",
    "    #   return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "    # else:\n",
    "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    return {'progress_bar': tensorboard_logs, 'log': tensorboard_logs }\n",
    "    \n",
    "\n",
    "  # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "  #   if self.trainer:\n",
    "  #     xm.optimizer_step(optimizer)\n",
    "  #   else:\n",
    "  #     optimizer.step()\n",
    "  #   optimizer.zero_grad()\n",
    "  #   self.lr_scheduler.step()\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    t_total = len(self.train_dataloader()) * self.trainer.max_epochs * self.trainer.limit_train_batches\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hyperparams.lr, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return [optimizer] #, [scheduler]\n",
    "\n",
    "  def _generate_step(self, batch):\n",
    "    generated_ids = self.model.generate(\n",
    "        batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        num_beams=self.num_beams,\n",
    "        max_length=1000,\n",
    "        temperature=self.temperature,\n",
    "        top_k=self.top_k,\n",
    "        top_p=self.top_p,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for g in generated_ids\n",
    "    ]\n",
    "    target = [\n",
    "        self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for t in batch[\"target_ids\"]\n",
    "    ]\n",
    "    return (preds, target)\n",
    "\n",
    "  def exact_match_accuracy(self, preds, target):\n",
    "    total = len(preds)\n",
    "    correct = sum([1 for pred, tgt in zip(preds, target) if pred == tgt])\n",
    "    return correct / total\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    preds, target = self._generate_step(batch)\n",
    "    loss = self._step(batch)\n",
    "    if self.test_flag == 'graphql':\n",
    "      accuracy = self.exact_match_accuracy(preds, target)\n",
    "      return {\"test_loss\": loss, \"test_accuracy\": torch.tensor(accuracy)}\n",
    "    else:\n",
    "      return {\"test_loss\": loss, \"preds\": preds, \"target\": target }\n",
    "\n",
    "  # def test_end(self, outputs):\n",
    "  #   return self.validation_end(outputs)\n",
    "\n",
    "  def test_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "    self.log(\"avg_test_loss\", avg_loss, prog_bar=True)\n",
    "\n",
    "    if self.test_flag == 'graphql':\n",
    "        avg_accuracy = torch.stack([x[\"test_accuracy\"] for x in outputs if \"test_accuracy\" in x]).mean()\n",
    "        self.log(\"avg_test_accuracy\", avg_accuracy, prog_bar=True)\n",
    "        return {\"avg_test_loss\": avg_loss, \"avg_test_accuracy\": avg_accuracy}\n",
    "\n",
    "    else:\n",
    "        output_test_predictions_file = os.path.join(os.getcwd(), \"test_predictions.txt\")\n",
    "        with open(output_test_predictions_file, \"w+\") as p_writer:\n",
    "            for output_batch in outputs:\n",
    "                p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n",
    "            p_writer.close()\n",
    "        return {\"avg_test_loss\": avg_loss}\n",
    "\n",
    "  def prepare_data(self):\n",
    "    if self.task == 'finetune':\n",
    "      self.train_dataset_g = TextToGraphQLDataset(self.tokenizer)\n",
    "      self.val_dataset_g = TextToGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "      self.test_dataset_g = TextToGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      self.train_dataset_s = SpiderDataset(self.tokenizer)\n",
    "      self.val_dataset_s = SpiderDataset(self.tokenizer, type_path='dev.json')\n",
    "      self.test_dataset_s = SpiderDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      self.train_dataset = ConcatDataset([self.train_dataset_g,self.train_dataset_s])\n",
    "      self.val_dataset = ConcatDataset([self.val_dataset_g, self.val_dataset_s])\n",
    "      # self.test_dataset = ConcatDataset([test_dataset_g, test_dataset_s])\n",
    "      if self.test_flag == 'graphql':\n",
    "        self.test_dataset = self.test_dataset_g\n",
    "      else:\n",
    "        self.test_dataset = self.test_dataset_s\n",
    "      \n",
    "    else:\n",
    "      train_dataset_g = MaskGraphQLDataset(self.tokenizer)\n",
    "      val_dataset_g = MaskGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      train_dataset_s = CoSQLMaskDataset(self.tokenizer)\n",
    "      val_dataset_s = CoSQLMaskDataset(self.tokenizer, type_path='cosql_dev.json')\n",
    "\n",
    "      self.train_dataset = ConcatDataset([train_dataset_g, train_dataset_s])\n",
    "      self.val_dataset = ConcatDataset([val_dataset_g,val_dataset_s])\n",
    "\n",
    "  @staticmethod\n",
    "  def custom_collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    collated_batch = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key in ['source_ids', 'target_ids']:\n",
    "            max_length = max([len(sample[key]) for sample in batch])\n",
    "            padded_tensors = [torch.cat([sample[key], torch.zeros(max_length - len(sample[key]), dtype=torch.long)], dim=0) for sample in batch]\n",
    "            collated_batch[key] = torch.stack(padded_tensors, dim=0)\n",
    "        else:\n",
    "            max_length = max([len(sample[key]) for sample in batch])\n",
    "            padded_tensors = [torch.cat([sample[key], torch.zeros(max_length - len(sample[key]), dtype=torch.long)], dim=0) for sample in batch]\n",
    "            collated_batch[key] = torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "    return collated_batch\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.custom_collate_fn, num_workers=0)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.custom_collate_fn, num_workers=0)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d261be605a785d63\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d261be605a785d63\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir lightning_logs/\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We initialize the T5MultiSPModel(hyperparams,batch_size=32)\n"
     ]
    }
   ],
   "source": [
    "hyperparams = argparse.Namespace(**{'lr': 0.0004365158322401656}) # for 3 epochs\n",
    "\n",
    "# # system = ConvBartSystem(dataset, train_sampler, batch_size=2)\n",
    "system = T5MultiSPModel(hyperparams,batch_size=32)\n",
    "print(\"We initialize the T5MultiSPModel(hyperparams,batch_size=32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"lightning_logs/\")\n",
    "# Pass the logger to the Trainer\n",
    "trainer = pl.Trainer(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/migration/migration.py:195: PossibleUserWarning: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "  rank_zero_warn(\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.4.9 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file checkpoints/Error_Mirror/training_checkpoint_synthetic_mirror_1500.json3.ckpt`\n"
     ]
    }
   ],
   "source": [
    "initial_training_checkpoint_path = f\"checkpoints/Error_Mirror/training_checkpoint_synthetic_mirror_1500.json3.ckpt\"\n",
    "\n",
    "# Load the best initial training model for testing\n",
    "system = system.load_from_checkpoint(initial_training_checkpoint_path, hyperparams=hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3827\n",
      "Number of data points: 554\n",
      "Number of data points: 2159\n",
      "Number of data points: 293\n"
     ]
    }
   ],
   "source": [
    "system.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps\n",
      "['{']\n"
     ]
    }
   ],
   "source": [
    "inputs = system.val_dataset[0]\n",
    "system.tokenizer.decode(inputs['source_ids'])\n",
    "\n",
    "if(use_gpu == True):\n",
    "  system.model = system.model.cuda()\n",
    "else:\n",
    "  system.model = system.model.cpu()\n",
    "generated_ids = system.model.generate(inputs['source_ids'].unsqueeze(0), num_beams=5, repetition_penalty=1.0, max_length=56, early_stopping=True)\n",
    "# # # summary_text = system.tokenizer.decode(generated_ids[0])\n",
    "\n",
    "hyps = [system.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "\n",
    "print(\"hyps\")\n",
    "print(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.4.9 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file checkpoints/Error_Mirror/fine_tuned_checkpoint_synthetic_mirror_1500.json3.ckpt`\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_checkpoint_path = f\"checkpoints/Error_Mirror/fine_tuned_checkpoint_synthetic_mirror_1500.json3.ckpt\"\n",
    "\n",
    "system = system.load_from_checkpoint(fine_tuning_checkpoint_path, hyperparams=hyperparams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt, schemaId):\n",
    "\n",
    "    input_string = system.test_dataset_g.get_question_with_schema(prompt, schemaId)\n",
    "\n",
    "    inputs = system.tokenizer.batch_encode_plus([input_string], max_length=1024, return_tensors='pt')['input_ids']\n",
    "\n",
    "    if(use_gpu == True):\n",
    "      generated_ids = system.model.generate(inputs.cuda(), num_beams=3, repetition_penalty=1.0, max_length=1000, early_stopping=True)\n",
    "    else:\n",
    "      generated_ids = system.model.generate(inputs, num_beams=3, repetition_penalty=1.0, max_length=1000, early_stopping=True)\n",
    "    hyps = [system.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "    dict_res = {\"prediction\": hyps[0]}\n",
    "    return dict_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphql import parse\n",
    "from graphql.language.ast import ObjectValueNode, StringValueNode\n",
    "\n",
    "problematic_queries_count = 0\n",
    "\n",
    "def are_queries_semantically_equivalent(query1, query2):\n",
    "    global problematic_queries_count\n",
    "\n",
    "    def extract_fields(selection_set):\n",
    "        fields = {}\n",
    "        for field in selection_set.selections:\n",
    "            if hasattr(field, \"selection_set\") and field.selection_set:\n",
    "                fields[field.name.value] = extract_fields(field.selection_set)\n",
    "            else:\n",
    "                fields[field.name.value] = None\n",
    "        return fields\n",
    "\n",
    "    def sort_query_fields(query_dict):\n",
    "        for key, value in query_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                query_dict[key] = sort_query_fields(value)\n",
    "        return dict(sorted(query_dict.items()))\n",
    "\n",
    "    def normalize_query(query):\n",
    "        return ''.join(query.split())\n",
    "\n",
    "    def extract_arguments(normalized_query):\n",
    "        args_start = normalized_query.find(\"(\")\n",
    "        args_end = normalized_query.rfind(\")\")\n",
    "        if args_start == -1 or args_end == -1:\n",
    "            return None\n",
    "        return normalized_query[args_start+1:args_end]\n",
    "\n",
    "    normalized_query1 = normalize_query(query1)\n",
    "    normalized_query2 = normalize_query(query2)\n",
    "\n",
    "    # Check if the arguments are identical\n",
    "    if extract_arguments(normalized_query1) != extract_arguments(normalized_query2):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        ast1 = parse(query1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing query1: {query1}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        problematic_queries_count += 1\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        ast2 = parse(query2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing query2: {query2}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        problematic_queries_count += 1\n",
    "        return False\n",
    "\n",
    "    query1_dict = extract_fields(ast1.definitions[0].selection_set)\n",
    "    query2_dict = extract_fields(ast2.definitions[0].selection_set)\n",
    "\n",
    "    sorted_query1 = sort_query_fields(query1_dict)\n",
    "    sorted_query2 = sort_query_fields(query2_dict)\n",
    "\n",
    "\n",
    "    return sorted_query1 == sorted_query2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: PASSED\n",
      "Test 2: PASSED\n",
      "Test 3: PASSED\n",
      "Test 4: PASSED\n",
      "Test 5: PASSED\n",
      "Test 6: PASSED\n",
      "Test 7: PASSED\n",
      "Test 8: PASSED\n",
      "Test 9: PASSED\n",
      "Test 10: PASSED\n",
      "Test 11: PASSED\n",
      "Test 12: PASSED\n",
      "Test 13: PASSED\n",
      "Test 14: PASSED\n",
      "Test 15: PASSED\n",
      "Test 16: PASSED\n",
      "Test 17: PASSED\n",
      "Test 18: PASSED\n",
      "Test 19: PASSED\n",
      "Test 20: PASSED\n",
      "Test 21: PASSED\n",
      "Test 22: PASSED\n",
      "Test 23: PASSED\n",
      "Test 24: PASSED\n",
      "\n",
      "Total tests: 24\n",
      "Passed tests: 24\n",
      "Failed tests: 0\n"
     ]
    }
   ],
   "source": [
    "original_queries = [\n",
    "    'query { matches_aggregate { aggregate { count } } }',\n",
    "    'query { cartoon ( order_by : { title : asc } ) { title } }', \n",
    "    'query { highschooler ( where : { name : { _eq : \\\"Kyle\\\" } } ) { id } }',\n",
    "    'query { stadium_aggregate { aggregate { avg { capacity } max { capacity } } } }',\n",
    "    'query { flights ( where : { destairport : { _eq : \"APG\" } } ) { flight no } }',\n",
    "    'query { singer ( where : { citizenship : { _neq : \\\"France\\\" } } ) { name } }',\n",
    "    'query { paragraphs ( where : { paragraph_text : { _eq : \\\"Korea\\\" } } ) { other_details } }',\n",
    "    'query { people ( where : { nationality : { _eq : \\\"Russia\\\" } } ) { name } }',\n",
    "    'query { documents ( where : { template : { template_type_code : { _eq : \\\"BK\\\" } } } ) { document_name } }',\n",
    "    'query { ship_aggregate ( where : { disposition_of_ship : { _eq : \\\"Captured\\\" } } ) { aggregate { count } } }',\n",
    "    'query { flights ( where : { airlineByAirline : { airline : { _eq : \\\"United Airlines\\\" } } } ) { flightno } }',\n",
    "    'query { votes_aggregate ( where : { state : { _eq : \\\"CA\\\" } } ) { aggregate { max { created } } } }',\n",
    "    'query { visitor ( limit : 1 , order_by : { visits_aggregate : { max : { num_of_ticket : desc_nulls_last } } } ) { name age } }',\n",
    "    'query { templates ( where : { ref_template_type : { template_type_description : { _eq : \\\"Presentation\\\" } } } ) { template_id } }',\n",
    "    'query { templates ( where : { _or : [ { template_type_code : { _eq : \"PP\" } } , { template_type_code : { _eq : \"PPT\" } }] } ) { template_id } }',\n",
    "    'query { rankings ( limit : 1 , order_by : { tours : desc_nulls_last } ) { player { country_code first_name } } }',\n",
    "    'query { cars_data ( limit : 1 , order_by : { accelerate : asc } , where : { car_name : { model : { _eq : \\\"volvo\\\" } } } ) { cylinders } }',\n",
    "    'query { singer_aggregate ( where : { country : { _eq : \\\"France\\\" } } ) { aggregate { avg { age } min { age } max { age } } } }',\n",
    "    'query { singer ( where : { song_name : { _like : \\\"%Hey%\\\" } } ) { name country } }',\n",
    "    'query { teacher ( where : { _or : [ { age : { _eq : \\\"32\\\" } } , { age : { _eq : \\\"33\\\" } } ] } ) { name } }',\n",
    "    'query { teacher ( where : { course_arranges : { course : { course : { _eq : \\\"Math\\\" } } } } ) { name } }',\n",
    "    'query { documents ( where : { document_name : { _eq : \\\"Robbin CV\\\" } } ) { document_id template_id document_description } }',\n",
    "    'query { airlines_aggregate ( where : { _and : { airline : { _eq : \\\"United Airlines\\\" } , flights : { destairport : { _eq : \\\"ASY\\\" } } } } ) { aggregate { count } } }',\n",
    "    'query { museum_aggregate ( where : { _or : [ { open_year : { _gt : \\\"2013\\\" } } , { open_year : { _lt : \\\"2008\\\" } } ] } ) { aggregate { count } } }'\n",
    "]\n",
    "\n",
    "semantically_equivalent_original_queries = [\n",
    "    'query { matches_aggregate { aggregate { count } } }',\n",
    "    'query{cartoon(order_by:{title:asc}){title}}',\n",
    "    'query { highschooler ( where : { name : { _eq : \\\"Kyle\\\" } } ){ id } }',\n",
    "    'query{stadium_aggregate{aggregate{avg{capacity} max{capacity}}}}',\n",
    "    'query { flights ( where : { destairport : { _eq : \\\"APG\\\" } } ) { flight no } }',\n",
    "    'query{singer(where:{citizenship:{_neq:\\\"France\\\"}}){name}}',\n",
    "    'query { paragraphs ( where : { paragraph_text : { _eq : \\\"Korea\\\" } } ) { other_details } }',\n",
    "    'query{people(where:{nationality:{_eq:\\\"Russia\\\"}}){name}}',\n",
    "    'query { documents ( where : { template : { template_type_code : { _eq : \\\"BK\\\" } } } ) { document_name } }',\n",
    "    'query{ship_aggregate(where:{disposition_of_ship:{_eq:\\\"Captured\\\"}}){aggregate{count}}}',\n",
    "    'query { flights ( where : { airlineByAirline : { airline : { _eq : \\\"United Airlines\\\" } } } ) { flightno } }',\n",
    "    'query{votes_aggregate(where:{state:{_eq:\\\"CA\\\"}}){aggregate{max{created}}}}',\n",
    "    'query { visitor ( limit : 1 , order_by : { visits_aggregate : { max : { num_of_ticket : desc_nulls_last } } } ) { age name } }',\n",
    "    'query{templates(where:{ref_template_type:{template_type_description:{_eq:\\\"Presentation\\\"}}}){template_id}}',\n",
    "    'query { templates ( where : { _or : [ { template_type_code : { _eq : \\\"PP\\\" } } , { template_type_code : { _eq : \\\"PPT\\\" } }] } ) { template_id } }',\n",
    "    'query{rankings(limit:1,order_by:{tours:desc_nulls_last}){player{first_name country_code}}}',\n",
    "    'query { cars_data ( limit : 1 , order_by : { accelerate : asc } , where : { car_name : { model : { _eq : \\\"volvo\\\" } } } ) { cylinders } }',\n",
    "    'query{singer_aggregate(where:{country:{_eq:\\\"France\\\"}}){aggregate{avg{age}min{age}max{age}}}}',\n",
    "    'query { singer ( where : { song_name : { _like : \\\"%Hey%\\\" } } ) { name country } }',\n",
    "    'query{teacher(where:{_or:[{age:{_eq:\\\"32\\\"}},{age:{_eq:\\\"33\\\"}}]}){name}}',\n",
    "    'query { teacher ( where : { course_arranges : { course : { course : { _eq : \\\"Math\\\" } } } } ) { name } }',\n",
    "    'query{documents(where:{document_name:{_eq:\\\"Robbin CV\\\"}}){ document_description document_id template_id }}',\n",
    "    'query { airlines_aggregate ( where : { _and : { airline : { _eq : \\\"United Airlines\\\" } , flights : { destairport : { _eq : \\\"ASY\\\" } } } } ) { aggregate { count } } }',\n",
    "\t'query { museum_aggregate ( where : { _or : [ { open_year : { _gt : \\\"2013\\\" } } , { open_year : { _lt : \\\"2008\\\" } } ] } ) { aggregate { count } } }'\n",
    "]\n",
    "\n",
    "original_queries_expect_failure = [\n",
    "    'query { students ( limit : 1 , order_by : { date_left : asc } ) { first_name middle_name last_name } }',\n",
    "    'query { students ( limit : 1 , order_by : { date_left : asc } ) { first_name middle_name last_name } }',\n",
    "    'query { cartoon ( order_by : { title : asc } ) { title } }',\n",
    "    'query { cartoon ( order_by : { title : asc } ) { title } }',\n",
    "    'query { countrylanguage_aggregate ( where : { _and : { country : { indepyear : { _lt : \\\"1930\\\" } } , isofficial : { _eq : \\\"T\\\" } } } ) { aggregate { count } } }',\n",
    "    'query { countrylanguage ( where : { language : { _neq : \\\"English\\\" } } , distinct_on : countrycode ) { countrycode } }',\n",
    "    'query { matches_aggregate ( where : { _and : { winner_hand : { _eq : \\\"L\\\" } , tourney_name : { _eq : \\\"WTA Championships\\\" } } } ) { aggregate { count } } }',\n",
    "    'query { matches_aggregate ( where : { _and : { winner_hand : { _eq : \\\"L\\\" } , tourney_name : { _eq : \\\"WTA Championships\\\" } } } ) { aggregate { count } } }',\n",
    "    'query { documents_aggregate { aggregate { count } } }',\n",
    "    'query { documents { document_id document_name document_description } }',\n",
    "    'query { people ( order_by : { name : asc } ) { name birth_date } }',\n",
    "    'query { ref_template_types ( where : { template_type_code : { _eq : \\\"AD\\\" } } ) { template_type_description } }',\n",
    "    'query { countrylanguage ( where : { _and : { country : { headofstate : { _eq : \\\"Beatrix\\\" } } , isofficial : { _eq : \\\"T\\\" } } } ) { language } }',\n",
    "    'query { templates ( where : { _or : [ { template_type_code : { _eq : \\\"PP\\\" } } , { template_type_code : { _eq : \\\"PPT\\\" } } ] } ) { template_id } }',\n",
    "]\n",
    "\n",
    "revised_queries_expect_failure = [\n",
    "    'query { students ( limit : 1 , order_by : { date_left : desc } ) { first_name middle_name last_name } }',\n",
    "    'query { students ( limit : 2 , order_by : { date_left : asc } ) { first_name middle_name last_name } }',\n",
    "    'query { cartoon ( order_by : { title : desc } ) { title } }',\n",
    "    'query { cartoon { title } }',\n",
    "    'query { countrylanguage_aggregate ( where : { _and : { country : { indepyear : { _gt : \\\"1930\\\" } } , isofficial : { _eq : \\\"T\\\" } } } ) { aggregate { count } } }',\n",
    "    'query { countrylanguage ( where : { language : { _neq : \\\"French\\\" } } , distinct_on : countrycode ) { countrycode } }',\n",
    "    'query { matches_aggregate ( where : { _and : { winner_hand : { _eq : \\\"R\\\" } , tourney_name : { _eq : \\\"WTA Championships\\\" } } } ) { aggregate { count } } }',\n",
    "    'query { matches_aggregate ( where : { _and : { winner_hand : { _eq : \\\"L\\\" } , tourney_name : { _eq : \\\"Australian Open\\\" } } } ) { aggregate { count } } }',\n",
    "    'query { documents_aggregate { aggregate { sum } } }',\n",
    "    'query { documents { document_id document_name } }',\n",
    "    'query { people ( order_by : { birth_date : desc } ) { name email } }',\n",
    "    'query { ref_template_types ( where : { template_type_code : { _neq : \"AD\" } } ) { template_type_code } }',\n",
    "    'query { countrylanguage ( where : { _or : [ { country : { headofstate : { eq : \\\"Beatrix\\\" } } }, { isofficial : { _neq : \\\"T\\\" } } ] } ) { language } }',\n",
    "    'query { templates ( where : { _and : [ { template_type_code : { _neq : \"PP\" } } , { template_type_code : { _neq : \"PPT\" } } ] } ) { template_id template_type_code } }'\n",
    "]\n",
    "\n",
    "def test_queries(original_queries, revised_queries):\n",
    "    total_tests = len(original_queries)\n",
    "    passed_tests = 0\n",
    "\n",
    "    for i in range(total_tests):\n",
    "        original_query = original_queries[i]\n",
    "        revised_query = revised_queries[i]\n",
    "\n",
    "        if are_queries_semantically_equivalent(original_query, revised_query):\n",
    "            print(f\"Test {i + 1}: PASSED\")\n",
    "            passed_tests += 1\n",
    "        else:\n",
    "            print(f\"Test {i + 1}: FAILED\")\n",
    "\n",
    "    print(f\"\\nTotal tests: {total_tests}\")\n",
    "    print(f\"Passed tests: {passed_tests}\")\n",
    "    print(f\"Failed tests: {total_tests - passed_tests}\")\n",
    "\n",
    "test_queries(original_queries, semantically_equivalent_original_queries)\n",
    "#test_queries(original_queries_expect_failure, revised_queries_expect_failure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def custom_test_df(df):\n",
    "    exact_match = []\n",
    "    semantic_match = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row[\"question\"]\n",
    "        schemaId = row[\"schemaId\"]\n",
    "        query = row[\"query\"]\n",
    "\n",
    "        prediction = predict(prompt, schemaId)[\"prediction\"]\n",
    "\n",
    "        exact_match.append(1 if prediction == query else 0)\n",
    "        df.loc[index, 'exact_match'] = 1 if prediction == query else 0\n",
    "\n",
    "        semantic_match.append(1 if are_queries_semantically_equivalent(prediction, query) else 0)\n",
    "        df.loc[index, 'semantic_match'] = 1 if are_queries_semantically_equivalent(prediction, query) else 0\n",
    "\n",
    "        # Add predicted query to the dataframe\n",
    "        df.loc[index, 'predicted_query'] = prediction\n",
    "\n",
    "    def calculate_metrics(match_results):\n",
    "        accuracy = accuracy_score(match_results, np.ones(len(match_results)))\n",
    "        return accuracy\n",
    "\n",
    "    # Calculate evaluation metrics for exact match\n",
    "    exact_accuracy = calculate_metrics(exact_match)\n",
    "\n",
    "    # Calculate evaluation metrics for semantic match\n",
    "    semantic_accuracy = calculate_metrics(semantic_match)\n",
    "\n",
    "    return exact_accuracy, semantic_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3827\n",
      "Number of data points: 554\n",
      "Number of data points: 554\n",
      "Number of data points: 7000\n",
      "Number of data points: 1034\n",
      "Number of data points: 1034\n"
     ]
    }
   ],
   "source": [
    "system.task = 'finetune'\n",
    "system.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the result\n",
      "{'prediction': 'query { ship_aggregate ( where : { ship_type : { _eq : \"Captured\" } } ) { aggregate { count } } }'}\n"
     ]
    }
   ],
   "source": [
    "hardcoded_schemaId = \"battle_death\"\n",
    "hardcoded_prompt = \"How many ships ended up being 'Captured'?\"\n",
    "\n",
    "result = predict(hardcoded_prompt, hardcoded_schemaId)\n",
    "print(\"this is the result\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data from file\n",
    "file_path = 'SPEGQL-dataset/dataset/dev.json'\n",
    "with open(file_path) as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "dev_df = pd.DataFrame(json_data)\n",
    "\n",
    "# Specify the order of columns\n",
    "dev_df = dev_df[['query', 'schemaId', 'question']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while parsing query1: query { cars_data_aggregate ( where : { cylinders : { _eq : 4 } } ) { aggregate { avg { mpg { weight } } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:109\n",
      "1 | query { cars_data_aggregate ( where : { cylinders : { _eq : 4 } } ) { aggregate { avg { mpg { weight } } } }\n",
      "  |                                                                                                             ^\n",
      "Error while parsing query1: query { cars_data_aggregate ( where : { cylinders : { _eq : 4 } } ) { aggregate { avg { mpg { weight } } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:109\n",
      "1 | query { cars_data_aggregate ( where : { cylinders : { _eq : 4 } } ) { aggregate { avg { mpg { weight } } } }\n",
      "  |                                                                                                             ^\n",
      "Error while parsing query1: query { teacher { name course { course { name } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:52\n",
      "1 | query { teacher { name course { course { name } } }\n",
      "  |                                                    ^\n",
      "Error while parsing query1: query { teacher { name course { course { name } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:52\n",
      "1 | query { teacher { name course { course { name } } }\n",
      "  |                                                    ^\n",
      "Error while parsing query1: query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:82\n",
      "1 | query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "  |                                                                                  ^\n",
      "Error while parsing query1: query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:82\n",
      "1 | query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "  |                                                                                  ^\n",
      "Error while parsing query1: query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:82\n",
      "1 | query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "  |                                                                                  ^\n",
      "Error while parsing query1: query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "Exception: Syntax Error: Expected Name, found <EOF>.\n",
      "\n",
      "GraphQL request:1:82\n",
      "1 | query { teacher ( order_by : { name : asc } ) { name course { course { name } } }\n",
      "  |                                                                                  ^\n",
      "Error while parsing query1: query { conductor { name orchestras { } }\n",
      "Exception: Syntax Error: Expected Name, found '}'.\n",
      "\n",
      "GraphQL request:1:39\n",
      "1 | query { conductor { name orchestras { } }\n",
      "  |                                       ^\n",
      "Error while parsing query1: query { conductor { name orchestras { } }\n",
      "Exception: Syntax Error: Expected Name, found '}'.\n",
      "\n",
      "GraphQL request:1:39\n",
      "1 | query { conductor { name orchestras { } }\n",
      "  |                                       ^\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.38086642599277976, 0.3844765342960289)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add two empty columns to the dataframe: exact match and semantic match\n",
    "\n",
    "dev_df['exact_match'] = \"\"\n",
    "dev_df['semantic_match'] = \"\"\n",
    "\n",
    "dev_df['exact_match'] = \"\"\n",
    "dev_df['semantic_match'] = \"\"\n",
    "\n",
    "custom_test_df(dev_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to next line\n",
    "pd.set_option('display.max_colwidth', None)  # Show the full content of each cell\n",
    "\n",
    "\n",
    "dev_df.to_csv('dev_df_improved_model.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesting Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>schemaId</th>\n",
       "      <th>question</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>predicted_query</th>\n",
       "      <th>nesting_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query { ship_aggregate ( where : { disposition_of_ship : { _eq : \"Captured\" } } ) { aggregate { count } } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>How many ships ended up being 'Captured'?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { ship_aggregate ( where : { name : { _eq : \"Captured\" } } ) { aggregate { count } } }</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>query { ship ( order_by : { name : desc } ) { name tonnage } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>List the name and tonnage ordered by in descending alphaetical order for the names.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { ship ( order_by : { name : desc } ) { name tonnage } }</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query { battle { name date } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>List the name, date and result of each battle.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { battle { name date result } }</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>query { death_aggregate { aggregate { max { killed } min { killed } } } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>What is maximum and minimum death toll caused each time?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { death_aggregate { aggregate { max { death } min { death_stall } } } }</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>query { death_aggregate { aggregate { avg { injured } } } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>What is the average number of injuries caused each time?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { death_aggregate { aggregate { avg { injured } , time_by_ship_id } } } }</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         query      schemaId                                                                             question exact_match semantic_match                                                                               predicted_query  nesting_level\n",
       "0  query { ship_aggregate ( where : { disposition_of_ship : { _eq : \"Captured\" } } ) { aggregate { count } } }  battle_death                                            How many ships ended up being 'Captured'?           0              0  query { ship_aggregate ( where : { name : { _eq : \"Captured\" } } ) { aggregate { count } } }              3\n",
       "1                                               query { ship ( order_by : { name : desc } ) { name tonnage } }  battle_death  List the name and tonnage ordered by in descending alphaetical order for the names.           1              1                                query { ship ( order_by : { name : desc } ) { name tonnage } }              2\n",
       "2                                                                               query { battle { name date } }  battle_death                                       List the name, date and result of each battle.           0              0                                                         query { battle { name date result } }              2\n",
       "3                                    query { death_aggregate { aggregate { max { killed } min { killed } } } }  battle_death                             What is maximum and minimum death toll caused each time?           0              0                 query { death_aggregate { aggregate { max { death } min { death_stall } } } }              4\n",
       "4                                                  query { death_aggregate { aggregate { avg { injured } } } }  battle_death                             What is the average number of injuries caused each time?           0              0               query { death_aggregate { aggregate { avg { injured } , time_by_ship_id } } } }              4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to calculate the nesting level of a query\n",
    "def nesting_level(query):\n",
    "    query = re.sub(r'\\s+', '', query)  # Remove whitespace\n",
    "    max_nesting = 0\n",
    "    current_nesting = 0\n",
    "\n",
    "    for char in query:\n",
    "        if char == '{':\n",
    "            current_nesting += 1\n",
    "            max_nesting = max(max_nesting, current_nesting)\n",
    "        elif char == '}':\n",
    "            current_nesting -= 1\n",
    "\n",
    "    return max_nesting\n",
    "\n",
    "# Calculate the nesting level for each query and add it as a new column\n",
    "dev_df['nesting_level'] = dev_df['query'].apply(nesting_level)\n",
    "# create a new column in the dataframe to represent the nesting level bucket\n",
    "\n",
    "test = False\n",
    "stats = False\n",
    "\n",
    "if test:\n",
    "    query1 = 'query{teacher(where:{_or:[{age:{_eq:\\\"32\\\"}},{age:{_eq:\\\"33\\\"}}]}){name}}'\n",
    "    print(nesting_level(query1))\n",
    "    query2 = 'query { templates ( where : { _and : [ { template_type_code : { _neq : \"PP\" } } , { template_type_code : { _neq : \"PPT\" } } ] } ) { template_id template_type_code } }'\n",
    "    print(nesting_level(query2))\n",
    "    query3 = 'query { countrylanguage ( where : { language : { _neq : \\\"French\\\" } } , distinct_on : countrycode ) { countrycode } }'\n",
    "    print(nesting_level(query3))\n",
    "    query4 = 'query { matches_aggregate { aggregate { count } } }'\n",
    "    print(nesting_level(query4))\n",
    "    query5 = 'query{singer_aggregate(where:{country:{_eq:\\\"France\\\"}}){aggregate{avg{age}min{age}max{age}}}}'\n",
    "    print(nesting_level(query5))\n",
    "\n",
    "if stats:\n",
    "    # calculate the mean, median, min, and max of nesting levels\n",
    "    mean_nesting = dev_df['nesting_level'].mean()\n",
    "    median_nesting = dev_df['nesting_level'].median()\n",
    "    min_nesting = dev_df['nesting_level'].min()\n",
    "    max_nesting = dev_df['nesting_level'].max()\n",
    "\n",
    "    print(f\"Mean nesting level: {mean_nesting}\")\n",
    "    print(f\"Median nesting level: {median_nesting}\")\n",
    "    print(f\"Minimum nesting level: {min_nesting}\")\n",
    "    print(f\"Maximum nesting level: {max_nesting}\")\n",
    "\n",
    "    # plot a histogram of the nesting levels\n",
    "    plt.hist(dev_df['nesting_level'], edgecolor='black')\n",
    "    plt.xlabel('Nesting Level')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Nesting Levels')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Nesting level 2 \" + str(dev_df[dev_df['nesting_level'] == 2].shape[0]))\n",
    "    print(\"Nesting level 3 \" + str(dev_df[dev_df['nesting_level'] == 3].shape[0]))\n",
    "    print(\"Nesting level 4 \" + str(dev_df[dev_df['nesting_level'] == 4].shape[0]))\n",
    "    print(\"Nesting level 5 \" + str(dev_df[dev_df['nesting_level'] == 5].shape[0]))\n",
    "    print(\"Nesting level 6 \" + str(dev_df[dev_df['nesting_level'] == 6].shape[0]))\n",
    "\n",
    "\n",
    "dev_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>schemaId</th>\n",
       "      <th>question</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>predicted_query</th>\n",
       "      <th>nesting_level</th>\n",
       "      <th>num_args</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query { ship_aggregate ( where : { disposition_of_ship : { _eq : \"Captured\" } } ) { aggregate { count } } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>How many ships ended up being 'Captured'?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { ship_aggregate ( where : { name : { _eq : \"Captured\" } } ) { aggregate { count } } }</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>query { ship ( order_by : { name : desc } ) { name tonnage } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>List the name and tonnage ordered by in descending alphaetical order for the names.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { ship ( order_by : { name : desc } ) { name tonnage } }</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query { battle { name date } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>List the name, date and result of each battle.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { battle { name date result } }</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>query { death_aggregate { aggregate { max { killed } min { killed } } } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>What is maximum and minimum death toll caused each time?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { death_aggregate { aggregate { max { death } min { death_stall } } } }</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>query { death_aggregate { aggregate { avg { injured } } } }</td>\n",
       "      <td>battle_death</td>\n",
       "      <td>What is the average number of injuries caused each time?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { death_aggregate { aggregate { avg { injured } , time_by_ship_id } } } }</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         query      schemaId                                                                             question exact_match semantic_match                                                                               predicted_query  nesting_level  num_args\n",
       "0  query { ship_aggregate ( where : { disposition_of_ship : { _eq : \"Captured\" } } ) { aggregate { count } } }  battle_death                                            How many ships ended up being 'Captured'?           0              0  query { ship_aggregate ( where : { name : { _eq : \"Captured\" } } ) { aggregate { count } } }              3         1\n",
       "1                                               query { ship ( order_by : { name : desc } ) { name tonnage } }  battle_death  List the name and tonnage ordered by in descending alphaetical order for the names.           1              1                                query { ship ( order_by : { name : desc } ) { name tonnage } }              2         1\n",
       "2                                                                               query { battle { name date } }  battle_death                                       List the name, date and result of each battle.           0              0                                                         query { battle { name date result } }              2         0\n",
       "3                                    query { death_aggregate { aggregate { max { killed } min { killed } } } }  battle_death                             What is maximum and minimum death toll caused each time?           0              0                 query { death_aggregate { aggregate { max { death } min { death_stall } } } }              4         0\n",
       "4                                                  query { death_aggregate { aggregate { avg { injured } } } }  battle_death                             What is the average number of injuries caused each time?           0              0               query { death_aggregate { aggregate { avg { injured } , time_by_ship_id } } } }              4         0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def count_graphql_arguments(query: str) -> int:\n",
    "    # Removing white spaces and escape characters\n",
    "    query = re.sub(r'\\s|\\\\', '', query)\n",
    "    \n",
    "    # Regular expression to find arguments in the query\n",
    "    pattern = re.compile(r'(\\{[^\\{]*\\})')\n",
    "    matches = pattern.findall(query)\n",
    "\n",
    "    # Counting the number of arguments\n",
    "    count = 0\n",
    "    for match in matches:\n",
    "        count += match.count(':')\n",
    "    return count\n",
    "\n",
    "dev_df['num_args'] = dev_df['query'].apply(count_graphql_arguments)\n",
    "\n",
    "test = True\n",
    "stats = False\n",
    "\n",
    "if test:\n",
    "    query1 = 'query{teacher(where:{_or:[{age:{_eq:\\\"32\\\"}},{age:{_eq:\\\"33\\\"}}]}){name}}'\n",
    "    print(count_graphql_arguments(query1))  # Output: 2\n",
    "    query2 = 'query { templates ( where : { _and : [ { template_type_code : { _neq : \"PP\" } } , { template_type_code : { _neq : \"PPT\" } } ] } ) { template_id template_type_code } }'\n",
    "    print(count_graphql_arguments(query2))  # Output: 2\n",
    "    query3 = 'query { countrylanguage ( where : { language : { _neq : \\\"French\\\" } } , distinct_on : countrycode ) { countrycode } }'\n",
    "    print(count_graphql_arguments(query3))  # Output: 2\n",
    "    query4 = 'query { matches_aggregate { aggregate { count } } }'\n",
    "    print(count_graphql_arguments(query4))  # Output: 0\n",
    "    query5 = 'query{singer_aggregate(where:{country:{_eq:\\\"France\\\"}}){aggregate{avg{age}min{age}max{age}}}}'\n",
    "    print(count_graphql_arguments(query5))  # Output: 1\n",
    "    query6 = 'query { tv_channel_aggregate ( where : { language : { _eq : \\\"English\\\" } } ) { aggregate { count } } }'\n",
    "    print(count_graphql_arguments(query6))  # Output: 1\n",
    "    query7 = '{ conductor ( where : { orchestras : { year_of_founded : { _gt : 2008.0 } } } ) { name } }'\n",
    "    print(count_graphql_arguments(query7))  # Output: 1\n",
    "    query8 = '{ flights_aggregate ( where : { sourceairport : { _eq : \\\"APG\\\" } } ) { aggregate { count } } }'\n",
    "    print(count_graphql_arguments(query8))  # Output: 1\n",
    "    query9 = '{ poker_player_aggregate ( where : { earnings : { _lt : 200000.0 } } ) { aggregate { max { final_table_made } } } }'\n",
    "    print(count_graphql_arguments(query9))  # Output: 1\n",
    "    query10 = '{ country_aggregate ( where : { _and : { continent : { _eq : \\\"Africa\\\" } , governmentform : { _eq : \\\"Republic\\\" } } } ) { aggregate { avg { lifeexpectancy } } } }'\n",
    "    print(count_graphql_arguments(query10))  # Output: 2\n",
    "    query11 = '{ matches_aggregate ( where : { _and : { winner_hand : { _eq : \\\"L\\\" } , tourney_name : { _eq : \\\"WTA Championships\\\" } } } ) { aggregate { count } } }'\n",
    "    print(count_graphql_arguments(query11))  # Output: 2\n",
    "    query12 =  '{ paragraphs_aggregate ( where : { document : { document_name : { _eq : \\\"Summer Show\\\" } } } ) { aggregate { count } } }'\n",
    "    print(count_graphql_arguments(query12))  # Output: 1\n",
    "\n",
    "if stats:\n",
    "    dev_df['num_args'].hist()\n",
    "\n",
    "    print(dev_df['num_args'].min())\n",
    "    print(dev_df['num_args'].max())\n",
    "\n",
    "    print(dev_df['num_args'].mean())\n",
    "    print(dev_df['num_args'].median())\n",
    "\n",
    "\n",
    "dev_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n",
      "/var/folders/jt/qjvfxd317_s79391s2jnf0w40000gn/T/ipykernel_1236/3019154319.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  schema_df = schema_df.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "weight_types = 1\n",
    "weight_fields = 1\n",
    "weight_input_objects = 1\n",
    "weight_relationships = 2\n",
    "weight_arguments = 1\n",
    "\n",
    "def analyze_schema_complexity(schema_json):\n",
    "    types_count = 0\n",
    "    fields_count = 0\n",
    "    input_objects_count = 0\n",
    "    relationships_count = 0\n",
    "    arguments_count = 0\n",
    "\n",
    "    for type_ in schema_json[\"__schema\"][\"types\"]:\n",
    "        types_count += 1\n",
    "\n",
    "        if type_[\"kind\"] == \"INPUT_OBJECT\":\n",
    "            input_objects_count += 1\n",
    "\n",
    "        if \"fields\" in type_ and type_[\"fields\"] is not None:\n",
    "            for field in type_[\"fields\"]:\n",
    "                fields_count += 1\n",
    "\n",
    "                if \"args\" in field:\n",
    "                    arguments_count += len(field[\"args\"])\n",
    "                    relationships_count += 1\n",
    "\n",
    "    complexity_score = (\n",
    "        (types_count * weight_types)\n",
    "        + (fields_count * weight_fields)\n",
    "        + (input_objects_count * weight_input_objects)\n",
    "        + (relationships_count * weight_relationships)\n",
    "        + (arguments_count * weight_arguments)\n",
    "    )\n",
    "\n",
    "    schema_length = len(json.dumps(schema_json))\n",
    "\n",
    "    return complexity_score, types_count, fields_count, input_objects_count, relationships_count, arguments_count, schema_length\n",
    "\n",
    "schemas_folder = 'SPEGQL-dataset/Schemas'\n",
    "schema_folders = os.listdir(schemas_folder)\n",
    "\n",
    "# Create an empty DataFrame to store the schema metrics\n",
    "data = {\n",
    "    'schemaId': [],\n",
    "    'schema_total_complexity': [],\n",
    "    'schema_types_count': [],\n",
    "    'schema_fields_count': [],\n",
    "    'schema_input_objects_count': [],\n",
    "    'schema_relationships_count': [],\n",
    "    'schema_arguments_count': [],\n",
    "    'schema_length': []\n",
    "}\n",
    "schema_df = pd.DataFrame(data)\n",
    "\n",
    "# Iterate over schema folders\n",
    "for schema_folder in schema_folders:\n",
    "    schema_path = os.path.join(schemas_folder, schema_folder)\n",
    "    if os.path.isdir(schema_path):\n",
    "        schema_json_file = os.path.join(schema_path, 'schema.json')\n",
    "        with open(schema_json_file, 'r') as file:\n",
    "            schema_json = json.load(file)\n",
    "            complexity_score, types_count, fields_count, input_objects_count, relationships_count, arguments_count, schema_length = analyze_schema_complexity(schema_json)\n",
    "            \n",
    "            # Create a new row with the calculated metrics\n",
    "            new_row = {\n",
    "                'schemaId': schema_folder,\n",
    "                'schema_total_complexity': complexity_score,\n",
    "                'schema_types_count': types_count,\n",
    "                'schema_fields_count': fields_count,\n",
    "                'schema_input_objects_count': input_objects_count,\n",
    "                'schema_relationships_count': relationships_count,\n",
    "                'schema_arguments_count': arguments_count,\n",
    "                'schema_length': schema_length\n",
    "            }\n",
    "            \n",
    "            # Append the new row to the DataFrame\n",
    "            schema_df = schema_df.append(new_row, ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity score: (21, 3, 5, 1, 2, 8, 475)\n"
     ]
    }
   ],
   "source": [
    "## Test \n",
    "\n",
    "# Sample GraphQL schema in JSON format\n",
    "sample_schema = {\n",
    "    \"__schema\": {\n",
    "        \"types\": [\n",
    "            {\n",
    "                \"kind\": \"OBJECT\",\n",
    "                \"name\": \"query_root\",\n",
    "                \"fields\": [\n",
    "                    {\n",
    "                        \"name\": \"author\",\n",
    "                        \"args\": [\n",
    "                            {\"name\": \"limit\"},\n",
    "                            {\"name\": \"offset\"},\n",
    "                            {\"name\": \"order_by\"},\n",
    "                            {\"name\": \"where\"},\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"author_aggregate\",\n",
    "                        \"args\": [\n",
    "                            {\"name\": \"distinct_on\"},\n",
    "                            {\"name\": \"limit\"},\n",
    "                            {\"name\": \"offset\"},\n",
    "                            {\"name\": \"order_by\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"kind\": \"OBJECT\",\n",
    "                \"name\": \"author\",\n",
    "                \"fields\": [\n",
    "                    {\"name\": \"id\"},\n",
    "                    {\"name\": \"name\"},\n",
    "                    {\"name\": \"birthdate\"},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"kind\": \"INPUT_OBJECT\",\n",
    "                \"name\": \"author_bool_exp\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Run the analyze_schema_complexity function with the sample schema\n",
    "complexity_score = analyze_schema_complexity(sample_schema)\n",
    "print(\"Complexity score:\", complexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the schema_df with the dev_df on the schemaId column\n",
    "\n",
    "dev_df = dev_df.merge(schema_df, on='schemaId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query                         0\n",
       "schemaId                      0\n",
       "question                      0\n",
       "exact_match                   0\n",
       "semantic_match                0\n",
       "predicted_query               0\n",
       "nesting_level                 0\n",
       "num_args                      0\n",
       "schema_total_complexity       0\n",
       "schema_types_count            0\n",
       "schema_fields_count           0\n",
       "schema_input_objects_count    0\n",
       "schema_relationships_count    0\n",
       "schema_arguments_count        0\n",
       "schema_length                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()\n",
    "\n",
    "# Check if there are any null values in the dataset\n",
    "\n",
    "dev_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                         query      schemaId                                                                             question exact_match semantic_match                                                                               predicted_query  nesting_level  num_args  schema_total_complexity  schema_types_count  schema_fields_count  schema_input_objects_count  schema_relationships_count  schema_arguments_count  schema_length  question_length  query_length\n",
      "0  query { ship_aggregate ( where : { disposition_of_ship : { _eq : \"Captured\" } } ) { aggregate { count } } }  battle_death                                            How many ships ended up being 'Captured'?           0              0  query { ship_aggregate ( where : { name : { _eq : \"Captured\" } } ) { aggregate { count } } }              3         1                   1008.0               129.0                229.0                        62.0                       229.0                   130.0       149243.0               41           107\n",
      "1                                               query { ship ( order_by : { name : desc } ) { name tonnage } }  battle_death  List the name and tonnage ordered by in descending alphaetical order for the names.           1              1                                query { ship ( order_by : { name : desc } ) { name tonnage } }              2         1                   1008.0               129.0                229.0                        62.0                       229.0                   130.0       149243.0               83            62\n",
      "2                                                                               query { battle { name date } }  battle_death                                       List the name, date and result of each battle.           0              0                                                         query { battle { name date result } }              2         0                   1008.0               129.0                229.0                        62.0                       229.0                   130.0       149243.0               46            30\n",
      "3                                    query { death_aggregate { aggregate { max { killed } min { killed } } } }  battle_death                             What is maximum and minimum death toll caused each time?           0              0                 query { death_aggregate { aggregate { max { death } min { death_stall } } } }              4         0                   1008.0               129.0                229.0                        62.0                       229.0                   130.0       149243.0               56            73\n",
      "4                                                  query { death_aggregate { aggregate { avg { injured } } } }  battle_death                             What is the average number of injuries caused each time?           0              0               query { death_aggregate { aggregate { avg { injured } , time_by_ship_id } } } }              4         0                   1008.0               129.0                229.0                        62.0                       229.0                   130.0       149243.0               56            59\n"
     ]
    }
   ],
   "source": [
    "## make a function that calculates the question length. It should do so by getting the length of the string in the question column\n",
    "\n",
    "def calculate_question_length(question):\n",
    "    return len(question)\n",
    "\n",
    "# Add a new column to the DataFrame with the question length. \n",
    "dev_df['question_length'] = dev_df['question'].apply(calculate_question_length)\n",
    "\n",
    "#make a function that calculates the query length\n",
    "\n",
    "def calculate_query_length(query):\n",
    "    return len(query)\n",
    "\n",
    "# Add a new column to the DataFrame with the query length\n",
    "dev_df['query_length'] = dev_df['query'].apply(calculate_query_length)\n",
    "\n",
    "print(dev_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7048a5d75593413cc54dc24206831079ac8905ffddb319c4eafd454be0ec5d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
