{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
      "  Referenced from: /Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      " in /Users/jakobtolstrup/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd83116b8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "from transformers import get_linear_schedule_with_warmup, AutoConfig \n",
    "from transformers import BartTokenizer,BartModel,BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model\n",
    "from transformers import BartConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.autograd import Variable\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import socket\n",
    "from os.path import basename\n",
    "from functools import reduce\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import sys\n",
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my version of transformers is 4.15.0\n",
      "my version of pytorch is 1.10.0\n",
      "my version of pytorch_lightning is 1.9.3\n"
     ]
    }
   ],
   "source": [
    "print(\"my version of transformers is \" + transformers.__version__)\n",
    "print (\"my version of pytorch is \" + torch.__version__)\n",
    "print(\"my version of pytorch_lightning is \" + pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### States\n",
    "test_state = False\n",
    "tensorflow_active = True\n",
    "use_gpu = False\n",
    "train_state = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToGraphQLDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train.json', block_size=102):\n",
    "        'Initialization'\n",
    "        super(TextToGraphQLDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        self.schema_ids = []\n",
    "        root_path = './SPEGQL-dataset/'\n",
    "        dataset_path = root_path + 'dataset/' + type_path\n",
    "\n",
    "        schemas_path = root_path + 'Schemas/'\n",
    "        schemas = glob.glob(schemas_path + '**/' + 'simpleSchema.json')\n",
    "\n",
    "        self.max_len = 0\n",
    "        self.name_to_schema = {}\n",
    "        for schema_path in schemas:\n",
    "           with open(schema_path, 'r', encoding='utf-8') as s:\n",
    "            \n",
    "             data = json.load(s)\n",
    "\n",
    "             type_field_tokens = [ ['<t>'] + [t['name']] + ['{'] + [ f['name'] for f in t['fields']] + ['}'] + ['</t>'] for t in data['types']]\n",
    "             type_field_flat_tokens = reduce(list.__add__, type_field_tokens)\n",
    "\n",
    "             arguments = [a['name']  for a in data['arguments']]\n",
    "             schema_tokens = type_field_flat_tokens + ['<a>'] + arguments + ['</a>']\n",
    "\n",
    "             path = Path(schema_path)\n",
    "             schema_name = basename(str(path.parent))\n",
    "\n",
    "             self.name_to_schema[schema_name] = schema_tokens\n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "\n",
    "          for element in data:\n",
    "            question_with_schema = 'translate English to GraphQL: ' + element['question']  + ' ' + ' '.join(self.name_to_schema[element['schemaId']])\n",
    "            tokenized_s = tokenizer.encode_plus(question_with_schema,max_length=1024, padding=True, truncation=True, return_tensors='pt')\n",
    "            self.source.append(tokenized_s)\n",
    "\n",
    "            tokenized_t = tokenizer.encode_plus(element['query'],max_length=block_size, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.target.append(tokenized_t)\n",
    "            self.schema_ids.append(element['schemaId'])\n",
    "\n",
    "  def get_question_with_schema(self, question, schemaId):\n",
    "        return 'translate English to GraphQL: ' + question  + ' ' + ' '.join(self.name_to_schema[schemaId])\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]['input_ids'].squeeze()\n",
    "        target_ids = self.target[index]['input_ids'].squeeze()\n",
    "        src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "\n",
    "        return { \n",
    "            'source_ids': source_ids,\n",
    "                'source_mask': src_mask,\n",
    "                'target_ids': target_ids,\n",
    "                'target_ids_y': target_ids\n",
    "                }\n",
    "\n",
    "sys.modules[\"__main__\"].TextToGraphQLDataset = TextToGraphQLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    dataset = TextToGraphQLDataset(tokenizer=tokenizer, type_path='train.json', block_size=102)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"TextToGraphQLDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGraphQLDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train.json', block_size=64):\n",
    "        'Initialization'\n",
    "        super(MaskGraphQLDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        path = './SPEGQL-dataset/dataset/' + type_path\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "          data = json.load(f)\n",
    "          for example in data:\n",
    "\n",
    "            utterance = example['query']\n",
    "            encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "            token_count = encoded_source.shape[0]\n",
    "            repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "            for pos in range(1, token_count):\n",
    "              encoded_source = repeated_utterance[pos].clone()\n",
    "              target_id = encoded_source[pos].item()\n",
    "              if target_id == tokenizer.eos_token_id:\n",
    "                  break\n",
    "              encoded_source[pos] = tokenizer.mask_token_id\n",
    "              decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "              encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze()\n",
    "              if encoded_target is not None and torch.numel(encoded_target) > 0:\n",
    "                  self.target.append(encoded_target)\n",
    "                  self.source.append(encoded_source)\n",
    "              if torch.numel(encoded_target) > 0:\n",
    "                  self.target.append(encoded_target)\n",
    "                  self.source.append(encoded_source)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]\n",
    "        target_id = self.target[index]\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = MaskGraphQLDataset(tokenizer=tokenizer, type_path='train.json', block_size=64)\n",
    "    print(\"MaskGraphQLDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train_spider.json', block_size=102):\n",
    "        'Initialization'\n",
    "        super(SpiderDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        spider_path = './spider/'\n",
    "        path = spider_path + type_path\n",
    "        # TODO open up tables.json\n",
    "        # its a list of tables\n",
    "        # group by db_id \n",
    "        # grab column name from column_names_original ( each column name is a list of two. and the 2nd index {1} is the column name )\n",
    "        # grab table names from table_names (^ same as above )\n",
    "        # concat both with the english question (table names + <c> + column names + <q> english question)\n",
    "        # tokenize\n",
    "\n",
    "        # Maybe try making making more structure \n",
    "        # in the concat by using primary_keys and foreign_keys \n",
    "\n",
    "        tables_path = spider_path + 'tables.json'\n",
    "\n",
    "        with open(path, 'r') as f, open(tables_path, 'r') as t:\n",
    "          databases = json.load(t)\n",
    "          data = json.load(f)\n",
    "\n",
    "          #groupby db_id \n",
    "          grouped_dbs = {}\n",
    "          for db in databases:\n",
    "            grouped_dbs[db['db_id']] = db\n",
    "          # print(grouped_dbs)\n",
    "          # end grop tables\n",
    "\n",
    "          for element in data:\n",
    "            db = grouped_dbs[element['db_id']]\n",
    "\n",
    "            # tables_names = \" \".join(db['table_names_original'])\n",
    "            db_tables = db['table_names_original']\n",
    "\n",
    "            # columns_names = \" \".join([column_name[1] for column_name in db['column_names_original'] ])\n",
    "            tables_with_columns = ''\n",
    "            for table_id, group in itertools.groupby(db['column_names_original'], lambda x: x[0]):\n",
    "              if table_id == -1:\n",
    "                continue\n",
    "\n",
    "              columns_names = \" \".join([column_name[1] for column_name in group ])\n",
    "              tables_with_columns += '<t> ' + db_tables[table_id] + ' <c> ' + columns_names + ' </c> ' + '</t> '\n",
    "\n",
    "\n",
    "            # group columns with tables. \n",
    "\n",
    "            db_with_question = 'translate English to SQL: ' + element['question'] + ' ' + tables_with_columns\n",
    "            # question_with_schema = 'translate English to GraphQL: ' + element['question']  + ' ' + ' '.join(self.name_to_schema[element['schemaId']]) + ' </s>'\n",
    "\n",
    "            tokenized_s = tokenizer.batch_encode_plus([db_with_question],max_length=1024, padding='max_length', truncation=True,return_tensors='pt')\n",
    "            # what is the largest example size?\n",
    "            # the alternative is to collate\n",
    "            #might need to collate\n",
    "            self.source.append(tokenized_s)\n",
    "\n",
    "            tokenized_t = tokenizer.batch_encode_plus([element['query']],max_length=block_size, padding='max_length', truncation=True,return_tensors='pt')\n",
    "            self.target.append(tokenized_t)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]['input_ids'].squeeze()\n",
    "        target_ids = self.target[index]['input_ids'].squeeze()\n",
    "        src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "        return { 'source_ids': source_ids,\n",
    "                'source_mask': src_mask,\n",
    "                'target_ids': target_ids,\n",
    "                'target_ids_y': target_ids}\n",
    "\n",
    "\n",
    "# # In[38]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    dataset = SpiderDataset(tokenizer=tokenizer , type_path='train_spider.json', block_size=102)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"SpiderDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoSQLMaskDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='cosql_train.json', block_size=64):\n",
    "        'Initialization'\n",
    "        super(CoSQLMaskDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        path = './cosql_dataset/sql_state_tracking/' + type_path\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "          for element in data:\n",
    "            for interaction in element['interaction']:\n",
    "              # repeat the squence for the amount of tokens. \n",
    "              # loop through those sequences and replace a different token in each one. \n",
    "              # the target will be that token. \n",
    "              utterance = interaction['query']\n",
    "              # tokens = utterance.split()\n",
    "              encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "              token_count = encoded_source.shape[0]\n",
    "              # print(encoded_source.shape)\n",
    "              repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "              for pos in range(1, token_count):\n",
    "                encoded_source = repeated_utterance[pos].clone()\n",
    "                target_id = encoded_source[pos].item()\n",
    "                if target_id == tokenizer.eos_token_id:\n",
    "                  break\n",
    "                # encoded_source[pos] = tokenizer.mask_token_id\n",
    "                # self.target.append(target_id)\n",
    "                # self.source.append(encoded_source)\n",
    "\n",
    "                encoded_source[pos] = tokenizer.mask_token_id\n",
    "                decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "                encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze() # should always be of size 1\n",
    "                self.target.append(encoded_target)\n",
    "                self.source.append(encoded_source)\n",
    "\n",
    "                # repeated_utterance[pos][pos] = target_token # so that the next iteration the previous token is correct\n",
    "\n",
    "                \n",
    "          \n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]#['input_ids'].squeeze()\n",
    "        target_id = self.target[index]#['input_ids'].squeeze()\n",
    "        # src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}\n",
    "                # 'source_mask': src_mask,\n",
    "                # 'target_ids': target_ids,\n",
    "                # 'target_ids_y': target_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = CoSQLMaskDataset(tokenizer=tokenizer , type_path='cosql_train.json', block_size=64)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"CoSQLMaskDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5MultiSPModel(pl.LightningModule):\n",
    "  def __init__(self, hyperparams, task='denoise', test_flag='graphql', train_sampler=None, batch_size=2,temperature=1.0,top_k=50, top_p=1.0, num_beams=1 ):\n",
    "    super(T5MultiSPModel, self).__init__()\n",
    "\n",
    "    self.temperature = temperature\n",
    "    self.top_k = top_k\n",
    "    self.top_p = top_p\n",
    "    self.num_beams = num_beams\n",
    "\n",
    "    self.hyperparams = hyperparams\n",
    "\n",
    "    self.task = task\n",
    "    self.test_flag = test_flag\n",
    "    self.train_sampler = train_sampler\n",
    "    self.batch_size = batch_size\n",
    "    if self.task == 'finetune':\n",
    "      self.model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    else: \n",
    "      self.model = T5ForConditionalGeneration.from_pretrained('t5-base') # no output past? \n",
    "\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    \n",
    "    self.criterion = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    self.add_special_tokens()\n",
    "\n",
    "  def forward(\n",
    "    self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "  def add_special_tokens(self):\n",
    "    # new special tokens\n",
    "    special_tokens_dict = self.tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    self.tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "  def _step(self, batch):\n",
    "    if self.task == 'finetune':\n",
    "      pad_token_id = self.tokenizer.pad_token_id\n",
    "      source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
    "      # y_ids = y[:, :-1].contiguous()\n",
    "      labels = y[:, :].clone()\n",
    "      labels[y[:, :] == pad_token_id] = -100\n",
    "      # attention_mask is for ignore padding on source_ids \n",
    "      # labels need to have pad_token ignored manually by setting to -100\n",
    "      # todo check the ignore token for forward\n",
    "      # seems like decoder_input_ids can be removed. \n",
    "      outputs = self(source_ids, attention_mask=source_mask, labels=labels,)\n",
    "\n",
    "      loss = outputs[0]\n",
    "\n",
    "    else: \n",
    "      y = batch['target_id']\n",
    "      labels = y[:, :].clone()\n",
    "      labels[y[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "      loss = self(\n",
    "          input_ids=batch[\"source_ids\"],\n",
    "          labels=labels\n",
    "      )[0]\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    tensorboard_logs = {\"train_loss\": loss}\n",
    "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    print(f'Validation step called, batch_idx: {batch_idx}, loss: {loss.item()}')\n",
    "\n",
    "    return {\"val_loss\": loss}\n",
    "\n",
    "\n",
    "  def on_validation_epoch_end(self, outputs=None):\n",
    "    if not outputs:\n",
    "        print(\"Empty outputs list.\")\n",
    "        return\n",
    "    print(\"outputs \" + str(outputs))\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    # if self.task == 'finetune':\n",
    "    #   avg_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "    #   tensorboard_logs = {\"val_loss\": avg_loss, \"avg_val_acc\": avg_acc}\n",
    "    #   return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "    # else:\n",
    "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    return {'progress_bar': tensorboard_logs, 'log': tensorboard_logs }\n",
    "    \n",
    "\n",
    "  # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "  #   if self.trainer:\n",
    "  #     xm.optimizer_step(optimizer)\n",
    "  #   else:\n",
    "  #     optimizer.step()\n",
    "  #   optimizer.zero_grad()\n",
    "  #   self.lr_scheduler.step()\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    t_total = len(self.train_dataloader()) * self.trainer.max_epochs * self.trainer.limit_train_batches\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hyperparams.lr, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return [optimizer] #, [scheduler]\n",
    "\n",
    "  def _generate_step(self, batch):\n",
    "    generated_ids = self.model.generate(\n",
    "        batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        num_beams=self.num_beams,\n",
    "        max_length=1000,\n",
    "        temperature=self.temperature,\n",
    "        top_k=self.top_k,\n",
    "        top_p=self.top_p,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for g in generated_ids\n",
    "    ]\n",
    "    target = [\n",
    "        self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for t in batch[\"target_ids\"]\n",
    "    ]\n",
    "    return (preds, target)\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    preds, target = self._generate_step(batch)\n",
    "    loss = self._step(batch)\n",
    "    if self.test_flag == 'graphql':\n",
    "      accuracy = exact_match.exact_match_accuracy(preds,target)\n",
    "      return {\"test_loss\": loss, \"test_accuracy\": torch.tensor(accuracy)}\n",
    "    else: \n",
    "      return {\"test_loss\": loss, \"preds\": preds, \"target\": target }\n",
    "\n",
    "  # def test_end(self, outputs):\n",
    "  #   return self.validation_end(outputs)\n",
    "\n",
    "\n",
    "  def test_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "    \n",
    "    if self.test_flag == 'graphql':\n",
    "      avg_acc = torch.stack([x[\"test_accuracy\"] for x in outputs]).mean()\n",
    "      tensorboard_logs = {\"test_loss\": avg_loss, \"test_acc\": avg_acc}\n",
    "      return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "\n",
    "    else:\n",
    "      output_test_predictions_file = os.path.join(os.getcwd(), \"test_predictions.txt\")\n",
    "      with open(output_test_predictions_file, \"w+\") as p_writer:\n",
    "          for output_batch in outputs:\n",
    "              p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n",
    "          p_writer.close()\n",
    "      tensorboard_logs = {\"test_loss\": avg_loss}\n",
    "      return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "\n",
    "  def prepare_data(self):\n",
    "    if self.task == 'finetune':\n",
    "      self.train_dataset_g = TextToGraphQLDataset(self.tokenizer)\n",
    "      self.val_dataset_g = TextToGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "      self.test_dataset_g = TextToGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      self.train_dataset_s = SpiderDataset(self.tokenizer)\n",
    "      self.val_dataset_s = SpiderDataset(self.tokenizer, type_path='dev.json')\n",
    "      self.test_dataset_s = SpiderDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      self.train_dataset = ConcatDataset([self.train_dataset_g,self.train_dataset_s])\n",
    "      self.val_dataset = ConcatDataset([self.val_dataset_g, self.val_dataset_s])\n",
    "      # self.test_dataset = ConcatDataset([test_dataset_g, test_dataset_s])\n",
    "      if self.test_flag == 'graphql':\n",
    "        self.test_dataset = self.test_dataset_g\n",
    "      else:\n",
    "        self.test_dataset = self.test_dataset_s\n",
    "      \n",
    "    else:\n",
    "      train_dataset_g = MaskGraphQLDataset(self.tokenizer)\n",
    "      val_dataset_g = MaskGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      train_dataset_s = CoSQLMaskDataset(self.tokenizer)\n",
    "      val_dataset_s = CoSQLMaskDataset(self.tokenizer, type_path='cosql_dev.json')\n",
    "\n",
    "      self.train_dataset = ConcatDataset([train_dataset_g, train_dataset_s])\n",
    "      self.val_dataset = ConcatDataset([val_dataset_g,val_dataset_s])\n",
    "\n",
    "  @staticmethod\n",
    "  def custom_collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    collated_batch = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key in ['source_ids', 'target_ids']:\n",
    "            max_length = max([len(sample[key]) for sample in batch])\n",
    "            padded_tensors = [torch.cat([sample[key], torch.zeros(max_length - len(sample[key]), dtype=torch.long)], dim=0) for sample in batch]\n",
    "            collated_batch[key] = torch.stack(padded_tensors, dim=0)\n",
    "        else:\n",
    "            max_length = max([len(sample[key]) for sample in batch])\n",
    "            padded_tensors = [torch.cat([sample[key], torch.zeros(max_length - len(sample[key]), dtype=torch.long)], dim=0) for sample in batch]\n",
    "            collated_batch[key] = torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "    return collated_batch\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.custom_collate_fn, num_workers=0)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.custom_collate_fn, num_workers=0)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2f4c55bc8350a8ee\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2f4c55bc8350a8ee\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir lightning_logs/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We initialize the T5MultiSPModel(hyperparams,batch_size=32)\n"
     ]
    }
   ],
   "source": [
    "hyperparams = argparse.Namespace(**{'lr': 0.0004365158322401656}) # for 3 epochs\n",
    "\n",
    "# # system = ConvBartSystem(dataset, train_sampler, batch_size=2)\n",
    "system = T5MultiSPModel(hyperparams,batch_size=32)\n",
    "print(\"We initialize the T5MultiSPModel(hyperparams,batch_size=32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"lightning_logs/\")\n",
    "# Pass the logger to the Trainer\n",
    "trainer = pl.Trainer(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights loaded from model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('model_weights.pth'):\n",
    "    system.load_state_dict(torch.load('model_weights.pth'))\n",
    "    print(\"model weights loaded from model_weights.pth\")\n",
    "\n",
    "else:\n",
    "    # If the weights file doesn't exist, train the model and save the weights after training\n",
    "    print(\"lets train this model!\")\n",
    "    if (use_gpu):\n",
    "      trainer = Trainer(accelerator='gpu', max_epochs=1, log_every_n_steps=1, limit_train_batches=0.2, gpus=1)\n",
    "    else:\n",
    "      trainer = Trainer(max_epochs=1, log_every_n_steps=1, limit_train_batches=0.2)\n",
    "    trainer.fit(system)\n",
    "    torch.save(system.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps\n",
      "['{']\n"
     ]
    }
   ],
   "source": [
    "inputs = system.val_dataset[0]\n",
    "system.tokenizer.decode(inputs['source_ids'])\n",
    "\n",
    "if(use_gpu == True):\n",
    "  system.model = system.model.cuda()\n",
    "else:\n",
    "  system.model = system.model.cpu()\n",
    "generated_ids = system.model.generate(inputs['source_ids'].unsqueeze(0), num_beams=5, repetition_penalty=1.0, max_length=56, early_stopping=True)\n",
    "# # # summary_text = system.tokenizer.decode(generated_ids[0])\n",
    "\n",
    "hyps = [system.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "\n",
    "print(\"hyps\")\n",
    "print(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is allready fine-tuned, loading weights...\n",
      "fine_tuned_model_weights.pth loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('fine_tuned_model_weights.pth'):\n",
    "    # Load the model weights if the file exists\n",
    "  print(\"Model is allready fine-tuned, loading weights...\")\n",
    "  system.load_state_dict(torch.load('fine_tuned_model_weights.pth'))\n",
    "  print(\"fine_tuned_model_weights.pth loaded\")\n",
    "\n",
    "else:\n",
    "  print(\"Let's fine-tune this model!\")\n",
    "  if(use_gpu):\n",
    "    trainer = Trainer(gpus=1, max_epochs=5, progress_bar_refresh_rate=1, val_check_interval=0.5)\n",
    "  else:\n",
    "    trainer = Trainer(max_epochs=5, progress_bar_refresh_rate=1, val_check_interval=0.5)\n",
    "  trainer.fit(system)\n",
    "  torch.save(system.state_dict(), 'fine_tuned_model_weights.pth')\n",
    "  \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt, schemaId):\n",
    "\n",
    "    if system.train_dataset_g.name_to_schema[schemaId] is not None:\n",
    "        input_string = system.train_dataset_g.get_question_with_schema(prompt, schemaId)\n",
    "    elif system.dev_dataset.name_to_schema[schemaId] is not None:\n",
    "        input_string = system.val_dataset_g.get_question_with_schema(prompt, schemaId)\n",
    "    #print(input_string)\n",
    "\n",
    "    inputs = system.tokenizer.batch_encode_plus([input_string], max_length=1024, return_tensors='pt')['input_ids']\n",
    "    #print(inputs.shape)\n",
    "\n",
    "    if(use_gpu == True):\n",
    "      generated_ids = system.model.generate(inputs.cuda(), num_beams=3, repetition_penalty=1.0, max_length=1000, early_stopping=True)\n",
    "    else:\n",
    "      generated_ids = system.model.generate(inputs, num_beams=3, repetition_penalty=1.0, max_length=1000, early_stopping=True)\n",
    "    hyps = [system.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "    dict_res = {\"prediction\": hyps[0]}\n",
    "    return dict_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from graphql import parse, print_ast\n",
    "\n",
    "def are_queries_semantically_same(query1, query2):\n",
    "    # We recursively extract the fields from the query\n",
    "    def extract_fields(selection_set):\n",
    "        return {\n",
    "            field.name.value: extract_fields(field.selection_set) if field.selection_set else None\n",
    "            for field in selection_set.selections\n",
    "        }\n",
    "    \n",
    "    # We sort the fields to make sure that the order of the fields doesn't matter\n",
    "    def sort_query_fields(query_dict):\n",
    "        for key, value in query_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                query_dict[key] = sort_query_fields(value)\n",
    "        return {k: v for k, v in sorted(query_dict.items())}\n",
    "    \n",
    "    try :\n",
    "        ast1 = parse(query1)\n",
    "        ast2 = parse(query2)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    query1_dict = extract_fields(ast1.definitions[0].selection_set)\n",
    "    query2_dict = extract_fields(ast2.definitions[0].selection_set)\n",
    "    \n",
    "    sorted_query1 = sort_query_fields(query1_dict)\n",
    "    sorted_query2 = sort_query_fields(query2_dict)\n",
    "\n",
    "    # We compare the two queries\n",
    "    return sorted_query1 == sorted_query2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "  query1 = \"\"\"\n",
    "  query {\n",
    "    user {\n",
    "      id\n",
    "      name\n",
    "      email\n",
    "      posts {\n",
    "        title\n",
    "        content\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "\n",
    "  query2 = \"\"\"\n",
    "  query {\n",
    "    user {\n",
    "      name\n",
    "      id\n",
    "      posts {\n",
    "        content\n",
    "        title\n",
    "      }\n",
    "      email\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "\n",
    "  query3 = \"\"\"\n",
    "  query {\n",
    "    user {\n",
    "      id\n",
    "      name\n",
    "      posts {\n",
    "        title\n",
    "        content\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \"\"\"\n",
    "\n",
    "print(are_queries_semantically_same(query1, query2))  # Should print True\n",
    "print(are_queries_semantically_same(query1, query3))  # Should print False\n",
    "\n",
    "query4 = \"\"\"\n",
    "query {\n",
    "matches_aggregate (where: { _and: { winner_hand: { _eq: \"L\" }, tourney_name: { _eq: \"WTA Championships\" } } }) {\n",
    "aggregate {\n",
    "count\n",
    "}\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "query5 = \"\"\"\n",
    "query {\n",
    "matches_aggregate (where: { _and: { tourney_name: { _eq: \"WTA Championships\" }, winner_hand: { _eq: \"L\" } } }) {\n",
    "aggregate {\n",
    "count\n",
    "}\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(are_queries_semantically_same(query1, query2))  # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def custom_test(dataset_path):\n",
    "    # Load the dev dataset\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dev_data = json.load(f)\n",
    "\n",
    "    exact_match = []\n",
    "    semantic_match = []\n",
    "\n",
    "    for example in dev_data:\n",
    "        prompt = example[\"question\"]\n",
    "        schemaId = example[\"schemaId\"]\n",
    "        query = example[\"query\"]\n",
    "\n",
    "        prediction = predict(prompt, schemaId)[\"prediction\"]\n",
    "\n",
    "        exact_match.append(1 if prediction == query else 0)\n",
    "        semantic_match.append(1 if are_queries_semantically_same(prediction, query) else 0)\n",
    "\n",
    "    def calculate_metrics(match_results):\n",
    "        accuracy = accuracy_score(match_results, np.ones(len(match_results)))\n",
    "        f1 = f1_score(match_results, np.ones(len(match_results)))\n",
    "        precision = precision_score(match_results, np.ones(len(match_results)))\n",
    "        recall = recall_score(match_results, np.ones(len(match_results)))\n",
    "        return accuracy, f1, precision, recall\n",
    "\n",
    "    # Calculate evaluation metrics for exact match\n",
    "    exact_accuracy, exact_f1, exact_precision, exact_recall = calculate_metrics(exact_match)\n",
    "\n",
    "    # Calculate evaluation metrics for semantic match\n",
    "    semantic_accuracy, semantic_f1, semantic_precision, semantic_recall = calculate_metrics(semantic_match)\n",
    "\n",
    "    print(\"Exact match:\")\n",
    "    print(f\"Accuracy: {exact_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nSemantic match:\")\n",
    "    print(f\"Accuracy: {semantic_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def custom_test_df(df):\n",
    "    exact_match = []\n",
    "    semantic_match = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row[\"question\"]\n",
    "        schemaId = row[\"schemaId\"]\n",
    "        query = row[\"query\"]\n",
    "\n",
    "        prediction = predict(prompt, schemaId)[\"prediction\"]\n",
    "\n",
    "        exact_match.append(1 if prediction == query else 0) ## Double check why this does not give me the same result as in the csv file\n",
    "        df.loc[index, 'exact_match'] = 1 if prediction == query else 0\n",
    "\n",
    "        semantic_match.append(1 if are_queries_semantically_same(prediction, query) else 0)\n",
    "        df.loc[index, 'semantic_match'] = 1 if are_queries_semantically_same(prediction, query) else 0\n",
    "\n",
    "    def calculate_metrics(match_results):\n",
    "        accuracy = accuracy_score(match_results, np.ones(len(match_results)))\n",
    "        f1 = f1_score(match_results, np.ones(len(match_results)))\n",
    "        precision = precision_score(match_results, np.ones(len(match_results)))\n",
    "        recall = recall_score(match_results, np.ones(len(match_results)))\n",
    "        return accuracy, f1, precision, recall\n",
    "\n",
    "    # Calculate evaluation metrics for exact match\n",
    "    exact_accuracy, exact_f1, exact_precision, exact_recall = calculate_metrics(exact_match)\n",
    "\n",
    "    # Calculate evaluation metrics for semantic match\n",
    "    semantic_accuracy, semantic_f1, semantic_precision, semantic_recall = calculate_metrics(semantic_match)\n",
    "\n",
    "    print(\"Exact match:\")\n",
    "    print(f\"Accuracy: {exact_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nSemantic match:\")\n",
    "    print(f\"Accuracy: {semantic_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.task = 'finetune'\n",
    "system.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the result\n",
      "{'prediction': 'query { ship_aggregate ( where : { tonnage : { _lt : \"Captured\" } } ) { aggregate { count } } }'}\n"
     ]
    }
   ],
   "source": [
    "hardcoded_schemaId = \"battle_death\"\n",
    "hardcoded_prompt = \"How many ships ended up being not 'Captured'?\"\n",
    "\n",
    "result = predict(hardcoded_prompt, hardcoded_schemaId)\n",
    "print(\"this is the result\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset_path = 'SPEGQL-dataset/dataset/dev.json'\n",
    "\n",
    "#custom_test(dev_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match:\n",
      "Accuracy: 0.3604\n",
      "F1-score: 0.5298\n",
      "Precision: 0.3604\n",
      "Recall: 1.0000\n",
      "\n",
      "Semantic match:\n",
      "Accuracy: 0.6396\n",
      "F1-score: 0.7802\n",
      "Precision: 0.6396\n",
      "Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe from dev_df.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dev_df = pd.read_csv('dev_df.csv')\n",
    "\n",
    "# Add two empty columns to the dataframe: exact match and semantic match\n",
    "\n",
    "dev_df['exact_match'] = \"\"\n",
    "dev_df['semantic_match'] = \"\"\n",
    "\n",
    "#custom_test_df(dev_df)\n",
    "\n",
    "# Save the dataframe to a csv file for later use. Call it dev_df__with_results.csv\n",
    "\n",
    "# dev_df.to_csv('dev_df_with_results.csv')\n",
    "\n",
    "# Create a dataframe from dev_df with 5% of the data\n",
    "\n",
    "dev_df_20 = dev_df.sample(frac=0.2)\n",
    "custom_test_df(dev_df_20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_20.head()\n",
    "\n",
    "# How many rows are in the dataframe?\n",
    "\n",
    "len(dev_df_20.index)\n",
    "\n",
    "# # How many exact matches are there?\n",
    "\n",
    "dev_df_20['exact_match'].sum()\n",
    "\n",
    "# # How many semantic matches are there?\n",
    "\n",
    "dev_df_20['semantic_match'].sum()\n",
    "\n",
    "dev_df_20.to_csv('dev_df_20_with_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Mixed Linear Model Regression Results\n",
      "===================================================================================================\n",
      "Model:            MixedLM Dependent Variable: np.log(semantic_match_mod / (1 - semantic_match_mod))\n",
      "No. Observations: 55      Method:             REML                                                 \n",
      "No. Groups:       19      Scale:              13.3816                                              \n",
      "Min. group size:  1       Log-Likelihood:     -161.8251                                            \n",
      "Max. group size:  8       Converged:          Yes                                                  \n",
      "Mean group size:  2.9                                                                              \n",
      "----------------------------------------------------------------------------------------------------------\n",
      "                             Coef.         Std.Err.          z           P>|z|        [0.025        0.975]\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "Intercept                     7.436           3.349         2.220        0.026         0.872        14.000\n",
      "question_length              -0.029           0.034        -0.847        0.397        -0.095         0.038\n",
      "nesting_level                -1.204           0.809        -1.489        0.137        -2.790         0.381\n",
      "num_args                      0.280           0.496         0.564        0.573        -0.692         1.252\n",
      "schema_complexity            -0.000           0.001        -0.343        0.731        -0.001         0.001\n",
      "Group Var                     5.715           1.439                                                       \n",
      "===================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Replace 1s with a value slightly below 1, and 0s with a value slightly above 0\n",
    "dev_df_10_percent[\"semantic_match_mod\"] = np.where(dev_df_10_percent[\"semantic_match\"] == 1, 0.99, 0.01)\n",
    "\n",
    "# Define the formula for the fixed effects\n",
    "fixed_effects_formula = \"np.log(semantic_match_mod / (1 - semantic_match_mod)) ~ question_length + nesting_level + num_args + schema_complexity\"\n",
    "\n",
    "# Fit the model\n",
    "mixed_logit_model = smf.mixedlm(formula=fixed_effects_formula, data=dev_df_10_percent, groups=dev_df_10_percent['schemaId'], re_formula=\"~1\")\n",
    "\n",
    "# Fit the model using the Restricted Maximum Likelihood (REML) method\n",
    "mixed_logit_result = mixed_logit_model.fit(reml=True, method='bfgs')\n",
    "\n",
    "# Print the summary\n",
    "print(mixed_logit_result.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have this function that predicts the corresponding query to a question.\n",
    "# I pass this data frame to the function. The data frame consists of the following columns:\n",
    "\n",
    "# I wish to know how well it performs on the the different properties: question_length_bucket, nesting level, num_args, schema length, schema complexity\n",
    "# Additionally, I wish to know how well it performs on the interactions of these properties. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "endog has evaluated to an array with multiple columns that has shape (55, 2). This occurs when the variable converted to endog is non-numeric (e.g., bool or str).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model_formula \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msemantic_match ~ num_args + schema_complexity + question_length + nesting_level\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m smf\u001b[39m.\u001b[39;49mmixedlm(model_formula, data\u001b[39m=\u001b[39;49mdev_df_10_percent, groups\u001b[39m=\u001b[39;49mdev_df_10_percent[\u001b[39m'\u001b[39;49m\u001b[39mschemaId\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mfit(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py:1046\u001b[0m, in \u001b[0;36mMixedLM.from_formula\u001b[0;34m(cls, formula, data, re_formula, vc_formula, subset, use_sparse, missing, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mexog_vc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m exog_vc\n\u001b[1;32m   1045\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mgroups\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m groups\n\u001b[0;32m-> 1046\u001b[0m mod \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(MixedLM, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49mfrom_formula(\n\u001b[1;32m   1047\u001b[0m     formula, data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1049\u001b[0m \u001b[39m# expand re names to account for pairs of RE\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m (param_names,\n\u001b[1;32m   1051\u001b[0m  exog_re_names,\n\u001b[1;32m   1052\u001b[0m  exog_re_names_full) \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39m_make_param_names(exog_re_names)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:206\u001b[0m, in \u001b[0;36mModel.from_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m max_endog \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_formula_max_endog\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m (max_endog \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         endog\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m endog\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m max_endog):\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mendog has evaluated to an array with multiple \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    207\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mcolumns that has shape \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m. This occurs when \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    208\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mthe variable converted to endog is non-numeric\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    209\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m (e.g., bool or str).\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(endog\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    210\u001b[0m \u001b[39mif\u001b[39;00m drop_cols \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(drop_cols) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    211\u001b[0m     cols \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m exog\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m x \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m drop_cols]\n",
      "\u001b[0;31mValueError\u001b[0m: endog has evaluated to an array with multiple columns that has shape (55, 2). This occurs when the variable converted to endog is non-numeric (e.g., bool or str)."
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Convert the schemaId to a categorical variable\n",
    "dev_df_10_percent['schemaId'] = pd.Categorical(dev_df_10_percent['schemaId'])\n",
    "\n",
    "# Define the formula for the model\n",
    "model_formula = \"semantic_match ~ num_args + schema_complexity + question_length + nesting_level\"\n",
    "\n",
    "# Fit the model\n",
    "model = smf.mixedlm(model_formula, data=dev_df_10_percent, groups=dev_df_10_percent['schemaId']).fit(method='lbfgs')\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd51bf327c04802b512fe352f9afa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TextToGraphQLDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# system.num_beams = 3\n",
    "# system.test_flag = 'graphql'\n",
    "# system.prepare_data()\n",
    "# trainer.test(system)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7048a5d75593413cc54dc24206831079ac8905ffddb319c4eafd454be0ec5d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
