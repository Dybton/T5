{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AdamW\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Variable\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X45sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jakobtolstrup/Desktop/Thesis/myvenv/T5-Carrera/T5_SP2.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggers\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorBoardLogger\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/__init__.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     _logger\u001b[39m.\u001b[39maddHandler(logging\u001b[39m.\u001b[39mStreamHandler())\n\u001b[1;32m     32\u001b[0m     _logger\u001b[39m.\u001b[39mpropagate \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseed\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning_fabric/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# In PyTorch 2.0+, setting this variable will force `torch.cuda.is_available()` and `torch.cuda.device_count()`\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# to use an NVML-based implementation that doesn't poison forks.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/issues/83973\u001b[39;00m\n\u001b[1;32m     20\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPYTORCH_NVML_BASED_CUDA_CHECK\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfabric\u001b[39;00m \u001b[39mimport\u001b[39;00m Fabric  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseed\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     26\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mFabric\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mseed_everything\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning_fabric/fabric.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply_func\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_to_collection\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m is_overridden\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrank_zero\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_warn\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning_utilities/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__about__\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401, F403\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menums\u001b[39;00m \u001b[39mimport\u001b[39;00m StrEnum\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimports\u001b[39;00m \u001b[39mimport\u001b[39;00m compare_version, module_available\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m is_overridden\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning_utilities/core/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"Core utilities.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menums\u001b[39;00m \u001b[39mimport\u001b[39;00m StrEnum\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimports\u001b[39;00m \u001b[39mimport\u001b[39;00m compare_version, module_available\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m is_overridden\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_utilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrank_zero\u001b[39;00m \u001b[39mimport\u001b[39;00m WarningCache\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/lightning_utilities/core/imports.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, List, Optional\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpkg_resources\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequirements\u001b[39;00m \u001b[39mimport\u001b[39;00m Requirement\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m Version\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/packaging/requirements.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Optional \u001b[39mas\u001b[39;00m TOptional, Set\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyparsing\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     Combine,\n\u001b[1;32m     12\u001b[0m     Literal \u001b[39mas\u001b[39;00m L,\n\u001b[1;32m     13\u001b[0m     Optional,\n\u001b[1;32m     14\u001b[0m     ParseException,\n\u001b[1;32m     15\u001b[0m     Regex,\n\u001b[1;32m     16\u001b[0m     Word,\n\u001b[1;32m     17\u001b[0m     ZeroOrMore,\n\u001b[1;32m     18\u001b[0m     originalTextFor,\n\u001b[1;32m     19\u001b[0m     stringEnd,\n\u001b[1;32m     20\u001b[0m     stringStart,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmarkers\u001b[39;00m \u001b[39mimport\u001b[39;00m MARKER_EXPR, Marker\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mspecifiers\u001b[39;00m \u001b[39mimport\u001b[39;00m LegacySpecifier, Specifier, SpecifierSet\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyparsing/__init__.py:122\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m _builtin_exprs \u001b[39mas\u001b[39;00m core_builtin_exprs\n\u001b[0;32m--> 122\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhelpers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhelpers\u001b[39;00m \u001b[39mimport\u001b[39;00m _builtin_exprs \u001b[39mas\u001b[39;00m helper_builtin_exprs\n\u001b[1;32m    125\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39municode\u001b[39;00m \u001b[39mimport\u001b[39;00m unicode_set, UnicodeRangeList, pyparsing_unicode \u001b[39mas\u001b[39;00m unicode\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyparsing/helpers.py:674\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    669\u001b[0m any_open_tag, any_close_tag \u001b[39m=\u001b[39m make_html_tags(\n\u001b[1;32m    670\u001b[0m     Word(alphas, alphanums \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_:\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mset_name(\u001b[39m\"\u001b[39m\u001b[39many tag\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    671\u001b[0m )\n\u001b[1;32m    673\u001b[0m _htmlEntityMap \u001b[39m=\u001b[39m {k\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m): v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m html\u001b[39m.\u001b[39mentities\u001b[39m.\u001b[39mhtml5\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 674\u001b[0m common_html_entity \u001b[39m=\u001b[39m Regex(\u001b[39m\"\u001b[39;49m\u001b[39m&(?P<entity>\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(_htmlEntityMap) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m);\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mset_name(\n\u001b[1;32m    675\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcommon HTML entity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m )\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplace_html_entity\u001b[39m(t):\n\u001b[1;32m    680\u001b[0m     \u001b[39m\"\"\"Helper parser action to replace common HTML entities with their special characters\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyparsing/core.py:2883\u001b[0m, in \u001b[0;36mRegex.__init__\u001b[0;34m(self, pattern, flags, as_group_list, as_match, asGroupList, asMatch)\u001b[0m\n\u001b[1;32m   2880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflags \u001b[39m=\u001b[39m flags\n\u001b[1;32m   2882\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2883\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mre \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mcompile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpattern, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflags)\n\u001b[1;32m   2884\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreString \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern\n\u001b[1;32m   2885\u001b[0m \u001b[39mexcept\u001b[39;00m sre_constants\u001b[39m.\u001b[39merror:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/re.py:252\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(pattern, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    251\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sre_compile\u001b[39m.\u001b[39misstring(pattern):\n\u001b[1;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    307\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    763\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 764\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    766\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/sre_parse.py:950\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    947\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m    949\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    951\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    952\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    953\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     state \u001b[39m=\u001b[39m State()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    442\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    445\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    446\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/sre_parse.py:836\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(err\u001b[39m.\u001b[39mmsg, \u001b[39mlen\u001b[39m(name) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    834\u001b[0m sub_verbose \u001b[39m=\u001b[39m ((verbose \u001b[39mor\u001b[39;00m (add_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    835\u001b[0m                \u001b[39mnot\u001b[39;00m (del_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 836\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    838\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmissing ), unterminated subpattern\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    839\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m start)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    441\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    442\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    444\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    445\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    446\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/sre_parse.py:529\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    526\u001b[0m     subpatternappend(code)\n\u001b[1;32m    528\u001b[0m \u001b[39melif\u001b[39;00m this \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m SPECIAL_CHARS:\n\u001b[0;32m--> 529\u001b[0m     subpatternappend((LITERAL, _ord(this)))\n\u001b[1;32m    531\u001b[0m \u001b[39melif\u001b[39;00m this \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     here \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from functools import partial\n",
    "from transformers import get_linear_schedule_with_warmup, AutoConfig \n",
    "from transformers import BartTokenizer,BartModel,BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model\n",
    "from transformers import BartConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.autograd import Variable\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import socket\n",
    "from os.path import basename\n",
    "from functools import reduce\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import sys\n",
    "import subprocess\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my version of transformers is 4.15.0\n",
      "my version of pytorch is 1.10.0\n",
      "my version of pytorch_lightning is 1.9.3\n"
     ]
    }
   ],
   "source": [
    "print(\"my version of transformers is \" + transformers.__version__)\n",
    "print (\"my version of pytorch is \" + torch.__version__)\n",
    "print(\"my version of pytorch_lightning is \" + pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### States\n",
    "test_state = False\n",
    "tensorflow_active = True\n",
    "use_gpu = False\n",
    "train_state = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToGraphQLDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train.json', block_size=102):\n",
    "        'Initialization'\n",
    "        super(TextToGraphQLDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        self.schema_ids = []\n",
    "        root_path = './SPEGQL-dataset/'\n",
    "        dataset_path = root_path + 'dataset/' + type_path\n",
    "\n",
    "        schemas_path = root_path + 'Schemas/'\n",
    "        schemas = glob.glob(schemas_path + '**/' + 'simpleSchema.json')\n",
    "\n",
    "        self.max_len = 0\n",
    "        self.name_to_schema = {}\n",
    "        for schema_path in schemas:\n",
    "           with open(schema_path, 'r', encoding='utf-8') as s:\n",
    "            \n",
    "             data = json.load(s)\n",
    "\n",
    "             type_field_tokens = [ ['<t>'] + [t['name']] + ['{'] + [ f['name'] for f in t['fields']] + ['}'] + ['</t>'] for t in data['types']]\n",
    "             type_field_flat_tokens = reduce(list.__add__, type_field_tokens)\n",
    "\n",
    "             arguments = [a['name']  for a in data['arguments']]\n",
    "             schema_tokens = type_field_flat_tokens + ['<a>'] + arguments + ['</a>']\n",
    "\n",
    "             path = Path(schema_path)\n",
    "             schema_name = basename(str(path.parent))\n",
    "\n",
    "             self.name_to_schema[schema_name] = schema_tokens\n",
    "\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "\n",
    "          for element in data:\n",
    "            question_with_schema = 'translate English to GraphQL: ' + element['question']  + ' ' + ' '.join(self.name_to_schema[element['schemaId']])\n",
    "            tokenized_s = tokenizer.encode_plus(question_with_schema,max_length=1024, padding=True, truncation=True, return_tensors='pt')\n",
    "            self.source.append(tokenized_s)\n",
    "\n",
    "            tokenized_t = tokenizer.encode_plus(element['query'],max_length=block_size, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.target.append(tokenized_t)\n",
    "            self.schema_ids.append(element['schemaId'])\n",
    "\n",
    "  def get_question_with_schema(self, question, schemaId):\n",
    "        return 'translate English to GraphQL: ' + question  + ' ' + ' '.join(self.name_to_schema[schemaId])\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]['input_ids'].squeeze()\n",
    "        target_ids = self.target[index]['input_ids'].squeeze()\n",
    "        src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "\n",
    "        return { \n",
    "            'source_ids': source_ids,\n",
    "                'source_mask': src_mask,\n",
    "                'target_ids': target_ids,\n",
    "                'target_ids_y': target_ids\n",
    "                }\n",
    "\n",
    "sys.modules[\"__main__\"].TextToGraphQLDataset = TextToGraphQLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    dataset = TextToGraphQLDataset(tokenizer=tokenizer, type_path='train.json', block_size=102)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"TextToGraphQLDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGraphQLDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train.json', block_size=64):\n",
    "        'Initialization'\n",
    "        super(MaskGraphQLDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        path = './SPEGQL-dataset/dataset/' + type_path\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "          data = json.load(f)\n",
    "          for example in data:\n",
    "\n",
    "            utterance = example['query']\n",
    "            encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "            token_count = encoded_source.shape[0]\n",
    "            repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "            for pos in range(1, token_count):\n",
    "              encoded_source = repeated_utterance[pos].clone()\n",
    "              target_id = encoded_source[pos].item()\n",
    "              if target_id == tokenizer.eos_token_id:\n",
    "                  break\n",
    "              encoded_source[pos] = tokenizer.mask_token_id\n",
    "              decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "              encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze()\n",
    "              if encoded_target is not None and torch.numel(encoded_target) > 0:\n",
    "                  self.target.append(encoded_target)\n",
    "                  self.source.append(encoded_source)\n",
    "              if torch.numel(encoded_target) > 0:\n",
    "                  self.target.append(encoded_target)\n",
    "                  self.source.append(encoded_source)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]\n",
    "        target_id = self.target[index]\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = MaskGraphQLDataset(tokenizer=tokenizer, type_path='train.json', block_size=64)\n",
    "    print(\"MaskGraphQLDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='train_spider.json', block_size=102):\n",
    "        'Initialization'\n",
    "        super(SpiderDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        spider_path = './spider/'\n",
    "        path = spider_path + type_path\n",
    "        # TODO open up tables.json\n",
    "        # its a list of tables\n",
    "        # group by db_id \n",
    "        # grab column name from column_names_original ( each column name is a list of two. and the 2nd index {1} is the column name )\n",
    "        # grab table names from table_names (^ same as above )\n",
    "        # concat both with the english question (table names + <c> + column names + <q> english question)\n",
    "        # tokenize\n",
    "\n",
    "        # Maybe try making making more structure \n",
    "        # in the concat by using primary_keys and foreign_keys \n",
    "\n",
    "        tables_path = spider_path + 'tables.json'\n",
    "\n",
    "        with open(path, 'r') as f, open(tables_path, 'r') as t:\n",
    "          databases = json.load(t)\n",
    "          data = json.load(f)\n",
    "\n",
    "          #groupby db_id \n",
    "          grouped_dbs = {}\n",
    "          for db in databases:\n",
    "            grouped_dbs[db['db_id']] = db\n",
    "          # print(grouped_dbs)\n",
    "          # end grop tables\n",
    "\n",
    "          for element in data:\n",
    "            db = grouped_dbs[element['db_id']]\n",
    "\n",
    "            # tables_names = \" \".join(db['table_names_original'])\n",
    "            db_tables = db['table_names_original']\n",
    "\n",
    "            # columns_names = \" \".join([column_name[1] for column_name in db['column_names_original'] ])\n",
    "            tables_with_columns = ''\n",
    "            for table_id, group in itertools.groupby(db['column_names_original'], lambda x: x[0]):\n",
    "              if table_id == -1:\n",
    "                continue\n",
    "\n",
    "              columns_names = \" \".join([column_name[1] for column_name in group ])\n",
    "              tables_with_columns += '<t> ' + db_tables[table_id] + ' <c> ' + columns_names + ' </c> ' + '</t> '\n",
    "\n",
    "\n",
    "            # group columns with tables. \n",
    "\n",
    "            db_with_question = 'translate English to SQL: ' + element['question'] + ' ' + tables_with_columns\n",
    "            # question_with_schema = 'translate English to GraphQL: ' + element['question']  + ' ' + ' '.join(self.name_to_schema[element['schemaId']]) + ' </s>'\n",
    "\n",
    "            tokenized_s = tokenizer.batch_encode_plus([db_with_question],max_length=1024, padding='max_length', truncation=True,return_tensors='pt')\n",
    "            # what is the largest example size?\n",
    "            # the alternative is to collate\n",
    "            #might need to collate\n",
    "            self.source.append(tokenized_s)\n",
    "\n",
    "            tokenized_t = tokenizer.batch_encode_plus([element['query']],max_length=block_size, padding='max_length', truncation=True,return_tensors='pt')\n",
    "            self.target.append(tokenized_t)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]['input_ids'].squeeze()\n",
    "        target_ids = self.target[index]['input_ids'].squeeze()\n",
    "        src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "        return { 'source_ids': source_ids,\n",
    "                'source_mask': src_mask,\n",
    "                'target_ids': target_ids,\n",
    "                'target_ids_y': target_ids}\n",
    "\n",
    "\n",
    "# # In[38]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    dataset = SpiderDataset(tokenizer=tokenizer , type_path='train_spider.json', block_size=102)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"SpiderDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoSQLMaskDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, tokenizer, type_path='cosql_train.json', block_size=64):\n",
    "        'Initialization'\n",
    "        super(CoSQLMaskDataset, ).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        path = './cosql_dataset/sql_state_tracking/' + type_path\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "          for element in data:\n",
    "            for interaction in element['interaction']:\n",
    "              # repeat the squence for the amount of tokens. \n",
    "              # loop through those sequences and replace a different token in each one. \n",
    "              # the target will be that token. \n",
    "              utterance = interaction['query']\n",
    "              # tokens = utterance.split()\n",
    "              encoded_source = tokenizer.encode(utterance, max_length=block_size, padding='max_length', truncation=True, return_tensors='pt').squeeze()\n",
    "              token_count = encoded_source.shape[0]\n",
    "              # print(encoded_source.shape)\n",
    "              repeated_utterance = [encoded_source for _ in range(token_count)]\n",
    "              for pos in range(1, token_count):\n",
    "                encoded_source = repeated_utterance[pos].clone()\n",
    "                target_id = encoded_source[pos].item()\n",
    "                if target_id == tokenizer.eos_token_id:\n",
    "                  break\n",
    "                # encoded_source[pos] = tokenizer.mask_token_id\n",
    "                # self.target.append(target_id)\n",
    "                # self.source.append(encoded_source)\n",
    "\n",
    "                encoded_source[pos] = tokenizer.mask_token_id\n",
    "                decoded_target = ''.join(tokenizer.convert_ids_to_tokens([target_id]))\n",
    "                encoded_target = tokenizer.encode(decoded_target, return_tensors='pt', max_length=4, padding='max_length', truncation=True).squeeze() # should always be of size 1\n",
    "                self.target.append(encoded_target)\n",
    "                self.source.append(encoded_source)\n",
    "\n",
    "                # repeated_utterance[pos][pos] = target_token # so that the next iteration the previous token is correct\n",
    "\n",
    "                \n",
    "          \n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.source)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        source_ids = self.source[index]#['input_ids'].squeeze()\n",
    "        target_id = self.target[index]#['input_ids'].squeeze()\n",
    "        # src_mask = self.source[index]['attention_mask'].squeeze()\n",
    "        return { 'source_ids': source_ids,\n",
    "                'target_id': target_id}\n",
    "                # 'source_mask': src_mask,\n",
    "                # 'target_ids': target_ids,\n",
    "                # 'target_ids_y': target_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_state:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "    special_tokens_dict = tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    print(tokenizer.mask_token)\n",
    "\n",
    "    dataset = CoSQLMaskDataset(tokenizer=tokenizer , type_path='cosql_train.json', block_size=64)\n",
    "\n",
    "    length = dataset.__len__()\n",
    "    item = dataset.__getitem__(0)\n",
    "    print(\"CoSQLMaskDataset test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5MultiSPModel(pl.LightningModule):\n",
    "  def __init__(self, hyperparams, task='denoise', test_flag='graphql', train_sampler=None, batch_size=2,temperature=1.0,top_k=50, top_p=1.0, num_beams=1 ):\n",
    "    super(T5MultiSPModel, self).__init__()\n",
    "\n",
    "    self.temperature = temperature\n",
    "    self.top_k = top_k\n",
    "    self.top_p = top_p\n",
    "    self.num_beams = num_beams\n",
    "\n",
    "    self.hyperparams = hyperparams\n",
    "\n",
    "    self.task = task\n",
    "    self.test_flag = test_flag\n",
    "    self.train_sampler = train_sampler\n",
    "    self.batch_size = batch_size\n",
    "    if self.task == 'finetune':\n",
    "      self.model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    else: \n",
    "      self.model = T5ForConditionalGeneration.from_pretrained('t5-base') # no output past? \n",
    "\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    \n",
    "    self.criterion = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    self.add_special_tokens()\n",
    "\n",
    "  def forward(\n",
    "    self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "    ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "  def add_special_tokens(self):\n",
    "    # new special tokens\n",
    "    special_tokens_dict = self.tokenizer.special_tokens_map # the issue could be here, might need to copy.\n",
    "    special_tokens_dict['mask_token'] = '<mask>'\n",
    "    special_tokens_dict['additional_special_tokens'] = ['<t>', '</t>', '<a>', '</a>']\n",
    "    self.tokenizer.add_tokens(['{', '}', '<c>', '</c>'])\n",
    "    self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "  def _step(self, batch):\n",
    "    if self.task == 'finetune':\n",
    "      pad_token_id = self.tokenizer.pad_token_id\n",
    "      source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
    "      # y_ids = y[:, :-1].contiguous()\n",
    "      labels = y[:, :].clone()\n",
    "      labels[y[:, :] == pad_token_id] = -100\n",
    "      # attention_mask is for ignore padding on source_ids \n",
    "      # labels need to have pad_token ignored manually by setting to -100\n",
    "      # todo check the ignore token for forward\n",
    "      # seems like decoder_input_ids can be removed. \n",
    "      outputs = self(source_ids, attention_mask=source_mask, labels=labels,)\n",
    "\n",
    "      loss = outputs[0]\n",
    "\n",
    "    else: \n",
    "      y = batch['target_id']\n",
    "      labels = y[:, :].clone()\n",
    "      labels[y[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "      loss = self(\n",
    "          input_ids=batch[\"source_ids\"],\n",
    "          labels=labels\n",
    "      )[0]\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    tensorboard_logs = {\"train_loss\": loss}\n",
    "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    print(f'Validation step called, batch_idx: {batch_idx}, loss: {loss.item()}')\n",
    "\n",
    "    return {\"val_loss\": loss}\n",
    "\n",
    "\n",
    "  def on_validation_epoch_end(self, outputs=None):\n",
    "    if not outputs:\n",
    "        print(\"Empty outputs list.\")\n",
    "        return\n",
    "    print(\"outputs \" + str(outputs))\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    # if self.task == 'finetune':\n",
    "    #   avg_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "    #   tensorboard_logs = {\"val_loss\": avg_loss, \"avg_val_acc\": avg_acc}\n",
    "    #   return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "    # else:\n",
    "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    return {'progress_bar': tensorboard_logs, 'log': tensorboard_logs }\n",
    "    \n",
    "\n",
    "  # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "  #   if self.trainer:\n",
    "  #     xm.optimizer_step(optimizer)\n",
    "  #   else:\n",
    "  #     optimizer.step()\n",
    "  #   optimizer.zero_grad()\n",
    "  #   self.lr_scheduler.step()\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    t_total = len(self.train_dataloader()) * self.trainer.max_epochs * self.trainer.limit_train_batches\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hyperparams.lr, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return [optimizer] #, [scheduler]\n",
    "\n",
    "  def _generate_step(self, batch):\n",
    "    generated_ids = self.model.generate(\n",
    "        batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        num_beams=self.num_beams,\n",
    "        max_length=1000,\n",
    "        temperature=self.temperature,\n",
    "        top_k=self.top_k,\n",
    "        top_p=self.top_p,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for g in generated_ids\n",
    "    ]\n",
    "    target = [\n",
    "        self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for t in batch[\"target_ids\"]\n",
    "    ]\n",
    "    return (preds, target)\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    preds, target = self._generate_step(batch)\n",
    "    loss = self._step(batch)\n",
    "    if self.test_flag == 'graphql':\n",
    "      accuracy = exact_match.exact_match_accuracy(preds,target)\n",
    "      return {\"test_loss\": loss, \"test_accuracy\": torch.tensor(accuracy)}\n",
    "    else: \n",
    "      return {\"test_loss\": loss, \"preds\": preds, \"target\": target }\n",
    "\n",
    "  # def test_end(self, outputs):\n",
    "  #   return self.validation_end(outputs)\n",
    "\n",
    "\n",
    "  def test_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "    \n",
    "    if self.test_flag == 'graphql':\n",
    "      avg_acc = torch.stack([x[\"test_accuracy\"] for x in outputs]).mean()\n",
    "      tensorboard_logs = {\"test_loss\": avg_loss, \"test_acc\": avg_acc}\n",
    "      return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "\n",
    "    else:\n",
    "      output_test_predictions_file = os.path.join(os.getcwd(), \"test_predictions.txt\")\n",
    "      with open(output_test_predictions_file, \"w+\") as p_writer:\n",
    "          for output_batch in outputs:\n",
    "              p_writer.writelines(s + \"\\n\" for s in output_batch[\"preds\"])\n",
    "          p_writer.close()\n",
    "      tensorboard_logs = {\"test_loss\": avg_loss}\n",
    "      return {\"progress_bar\": tensorboard_logs, \"log\": tensorboard_logs}\n",
    "\n",
    "  def prepare_data(self):\n",
    "    if self.task == 'finetune':\n",
    "      self.train_dataset_g = TextToGraphQLDataset(self.tokenizer)\n",
    "      self.val_dataset_g = TextToGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "      self.test_dataset_g = TextToGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      self.train_dataset_s = SpiderDataset(self.tokenizer)\n",
    "      self.val_dataset_s = SpiderDataset(self.tokenizer, type_path='dev.json')\n",
    "      self.test_dataset_s = SpiderDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      self.train_dataset = ConcatDataset([self.train_dataset_g,self.train_dataset_s])\n",
    "      self.val_dataset = ConcatDataset([self.val_dataset_g, self.val_dataset_s])\n",
    "      # self.test_dataset = ConcatDataset([test_dataset_g, test_dataset_s])\n",
    "      if self.test_flag == 'graphql':\n",
    "        self.test_dataset = self.test_dataset_g\n",
    "      else:\n",
    "        self.test_dataset = self.test_dataset_s\n",
    "      \n",
    "    else:\n",
    "      train_dataset_g = MaskGraphQLDataset(self.tokenizer)\n",
    "      val_dataset_g = MaskGraphQLDataset(self.tokenizer, type_path='dev.json')\n",
    "\n",
    "      train_dataset_s = CoSQLMaskDataset(self.tokenizer)\n",
    "      val_dataset_s = CoSQLMaskDataset(self.tokenizer, type_path='cosql_dev.json')\n",
    "\n",
    "      self.train_dataset = ConcatDataset([train_dataset_g, train_dataset_s])\n",
    "      self.val_dataset = ConcatDataset([val_dataset_g,val_dataset_s])\n",
    "\n",
    "  @staticmethod\n",
    "  def custom_collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    collated_batch = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key in ['source_ids', 'target_ids']:\n",
    "            max_length = max([len(sample[key]) for sample in batch])\n",
    "            padded_tensors = [torch.cat([sample[key], torch.zeros(max_length - len(sample[key]), dtype=torch.long)], dim=0) for sample in batch]\n",
    "            collated_batch[key] = torch.stack(padded_tensors, dim=0)\n",
    "        else:\n",
    "            max_length = max([len(sample[key]) for sample in batch])\n",
    "            padded_tensors = [torch.cat([sample[key], torch.zeros(max_length - len(sample[key]), dtype=torch.long)], dim=0) for sample in batch]\n",
    "            collated_batch[key] = torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "    return collated_batch\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.custom_collate_fn, num_workers=0)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.custom_collate_fn, num_workers=0)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-87a71d421edadc29\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-87a71d421edadc29\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# %load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir lightning_logs/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We initialize the T5MultiSPModel(hyperparams,batch_size=32)\n"
     ]
    }
   ],
   "source": [
    "hyperparams = argparse.Namespace(**{'lr': 0.0004365158322401656}) # for 3 epochs\n",
    "\n",
    "# # system = ConvBartSystem(dataset, train_sampler, batch_size=2)\n",
    "system = T5MultiSPModel(hyperparams,batch_size=32)\n",
    "print(\"We initialize the T5MultiSPModel(hyperparams,batch_size=32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the logger\n",
    "logger = TensorBoardLogger(\"lightning_logs/\")\n",
    "# Pass the logger to the Trainer\n",
    "trainer = pl.Trainer(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights loaded from model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('model_weights.pth'):\n",
    "    system.load_state_dict(torch.load('model_weights.pth'))\n",
    "    print(\"model weights loaded from model_weights.pth\")\n",
    "\n",
    "else:\n",
    "    # If the weights file doesn't exist, train the model and save the weights after training\n",
    "    print(\"lets train this model!\")\n",
    "    if (use_gpu):\n",
    "      trainer = Trainer(accelerator='gpu', max_epochs=1, log_every_n_steps=1, limit_train_batches=0.2, gpus=1)\n",
    "    else:\n",
    "      trainer = Trainer(max_epochs=1, log_every_n_steps=1, limit_train_batches=0.2)\n",
    "    trainer.fit(system)\n",
    "    torch.save(system.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps\n",
      "['{']\n"
     ]
    }
   ],
   "source": [
    "inputs = system.val_dataset[0]\n",
    "system.tokenizer.decode(inputs['source_ids'])\n",
    "\n",
    "if(use_gpu == True):\n",
    "  system.model = system.model.cuda()\n",
    "else:\n",
    "  system.model = system.model.cpu()\n",
    "generated_ids = system.model.generate(inputs['source_ids'].unsqueeze(0), num_beams=5, repetition_penalty=1.0, max_length=56, early_stopping=True)\n",
    "# # # summary_text = system.tokenizer.decode(generated_ids[0])\n",
    "\n",
    "hyps = [system.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "\n",
    "print(\"hyps\")\n",
    "print(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is allready fine-tuned, loading weights...\n",
      "fine_tuned_model_weights.pth loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('fine_tuned_model_weights.pth'):\n",
    "    # Load the model weights if the file exists\n",
    "  print(\"Model is allready fine-tuned, loading weights...\")\n",
    "  system.load_state_dict(torch.load('fine_tuned_model_weights.pth'))\n",
    "  print(\"fine_tuned_model_weights.pth loaded\")\n",
    "\n",
    "else:\n",
    "  print(\"Let's fine-tune this model!\")\n",
    "  if(use_gpu):\n",
    "    trainer = Trainer(gpus=1, max_epochs=5, progress_bar_refresh_rate=1, val_check_interval=0.5)\n",
    "  else:\n",
    "    trainer = Trainer(max_epochs=5, progress_bar_refresh_rate=1, val_check_interval=0.5)\n",
    "  trainer.fit(system)\n",
    "  torch.save(system.state_dict(), 'fine_tuned_model_weights.pth')\n",
    "  \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt, schemaId):\n",
    "\n",
    "    if system.train_dataset_g.name_to_schema[schemaId] is not None:\n",
    "        input_string = system.train_dataset_g.get_question_with_schema(prompt, schemaId)\n",
    "    elif system.dev_dataset.name_to_schema[schemaId] is not None:\n",
    "        input_string = system.val_dataset_g.get_question_with_schema(prompt, schemaId)\n",
    "    #print(input_string)\n",
    "\n",
    "    inputs = system.tokenizer.batch_encode_plus([input_string], max_length=1024, return_tensors='pt')['input_ids']\n",
    "    #print(inputs.shape)\n",
    "\n",
    "    if(use_gpu == True):\n",
    "      generated_ids = system.model.generate(inputs.cuda(), num_beams=3, repetition_penalty=1.0, max_length=1000, early_stopping=True)\n",
    "    else:\n",
    "      generated_ids = system.model.generate(inputs, num_beams=3, repetition_penalty=1.0, max_length=1000, early_stopping=True)\n",
    "    hyps = [system.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
    "    dict_res = {\"prediction\": hyps[0]}\n",
    "    return dict_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from graphql import parse, print_ast\n",
    "from graphql.language.ast import ObjectValueNode, ListValueNode\n",
    "\n",
    "def are_queries_semantically_same(query1, query2):\n",
    "    def extract_value(arg_value):\n",
    "        if isinstance(arg_value, ObjectValueNode):\n",
    "            return extract_object_fields(arg_value)\n",
    "        elif isinstance(arg_value, ListValueNode):\n",
    "            return [extract_value(item) for item in arg_value.values]\n",
    "        else:\n",
    "            return arg_value.value\n",
    "\n",
    "    def extract_fields(selection_set):\n",
    "        return {\n",
    "            field.name.value: {\n",
    "                'fields': extract_fields(field.selection_set) if field.selection_set else None,\n",
    "                'arguments': {arg.name.value: extract_value(arg.value) for arg in field.arguments}\n",
    "            } for field in selection_set.selections\n",
    "        }\n",
    "\n",
    "    def extract_object_fields(obj):\n",
    "        return {field.name.value: extract_value(field.value) for field in obj.fields}\n",
    "\n",
    "    def sort_query_fields(query_dict):\n",
    "        for key, value in query_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                query_dict[key] = {\n",
    "                    'fields': sort_query_fields(value['fields']) if value['fields'] else None,\n",
    "                    'arguments': {k: v for k, v in sorted(value['arguments'].items())}\n",
    "                }\n",
    "        return {k: v for k, v in sorted(query_dict.items())}\n",
    "\n",
    "    def normalize_and_conditions(arguments):\n",
    "        if '_and' in arguments:\n",
    "            conditions = arguments['_and']\n",
    "            if isinstance(conditions, list):\n",
    "                for i, condition in enumerate(conditions):\n",
    "                    conditions[i] = normalize_and_conditions(condition)\n",
    "                conditions.sort(key=lambda x: json.dumps(x, sort_keys=True))\n",
    "        return arguments\n",
    "\n",
    "    try:\n",
    "        ast1 = parse(query1)\n",
    "        ast2 = parse(query2)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    query1_dict = extract_fields(ast1.definitions[0].selection_set)\n",
    "    query2_dict = extract_fields(ast2.definitions[0].selection_set)\n",
    "\n",
    "    query1_dict = normalize_and_conditions(query1_dict)\n",
    "    query2_dict = normalize_and_conditions(query2_dict)\n",
    "\n",
    "    sorted_query1 = sort_query_fields(query1_dict)\n",
    "    sorted_query2 = sort_query_fields(query2_dict)\n",
    "\n",
    "    return sorted_query1 == sorted_query2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 - expected result: True\n",
      "True\n",
      "1 3 - expected result: False\n",
      "False\n",
      "4 5 - expected result: True\n",
      "True\n",
      "6 7 - expected result: True\n",
      "True\n",
      "8 9 - expected result: True\n",
      "True\n",
      "10 11 - expected result: False\n",
      "False\n",
      "13 14 expected result: True\n",
      "True\n",
      "15 16 expected result: True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "query1 = 'query { user { id name email posts { title content } } }'\n",
    "query2 = 'query { user { name id posts { content title } email } }'\n",
    "query3 = 'query { user { id name posts { title content } } }'\n",
    "\n",
    "print(\"1 2 - expected result: True\")\n",
    "print(are_queries_semantically_same(query1, query2))  # Should print True\n",
    "print(\"1 3 - expected result: False\")\n",
    "print(are_queries_semantically_same(query1, query3))  # Should print False\n",
    "\n",
    "query4 = 'query { matches_aggregate(where: {_and: {winner_hand: {_eq: \"L\"}, tourney_name: {_eq: \"WTA Championships\"}}}) { aggregate { count } } }'\n",
    "query5 = 'query { matches_aggregate(where: {_and: {tourney_name: {_eq: \"WTA Championships\"}, winner_hand: {_eq: \"L\"}}}) { aggregate { count } } }'\n",
    "\n",
    "print(\"4 5 - expected result: True\")\n",
    "print(are_queries_semantically_same(query4, query5))  # Should print True\n",
    "\n",
    "query6 = 'query { airports(where: {city: {_eq: \"Anthony\"}}) { airportcode airportname } }'\n",
    "query7 = 'query { airports(where: {city: {_eq: \"Anthony\"}}) { airportname airportcode } }'\n",
    "\n",
    "print(\"6 7 - expected result: True\")\n",
    "print(are_queries_semantically_same(query6, query7))  # Output: True\n",
    "\n",
    "query8 = 'query { stadium(where: {capacity: {_gte: 5000, _lte: 10000}}) { name location } }'\n",
    "query9 = 'query { stadium(where: {capacity: {_lte: 10000, _gte: 5000}}) { name location } }'\n",
    "\n",
    "print(\"8 9 - expected result: True\")\n",
    "print(are_queries_semantically_same(query8, query9))  # Output: True\n",
    "\n",
    "query10 = 'query { user(where: {id: {_eq: 10}}) { id name email posts { title content } } }'\n",
    "query11 = 'query { user { name email posts { title content } id } }'\n",
    "query12 = 'query { user { id name posts { content title } } }'\n",
    "\n",
    "print(\"10 11 - expected result: False\")\n",
    "print(are_queries_semantically_same(query10, query11))\n",
    "\n",
    "query13 = 'query { airports(where: {city: {_eq: \"Anthony\"}}) { airportname airportcode } }'\n",
    "query14 = 'query { airports(where: {city: {_eq: \"Anthony\"}}) { airportcode airportname } }'\n",
    "\n",
    "# Test case 4: Queries with different field order\n",
    "print(\"13 14 expected result: True\")\n",
    "print(are_queries_semantically_same(query13, query14))  # Should print True\n",
    "\n",
    "query15 = 'query { stadium(where: {capacity: {_gte: 5000, _lte: 10000}}) { location name } }'\n",
    "query16 = 'query { stadium(where: {capacity: {_lte: 10000, _gte: 5000}}) { name location } }'\n",
    "\n",
    "# Test case 5: Queries with different order in conditions\n",
    "print(\"15 16 expected result: True\")\n",
    "print(are_queries_semantically_same(query15, query16))  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queries</th>\n",
       "      <th>queries_shifted_order</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query { hiring { employee_id is_full_time shop_id start_from } }</td>\n",
       "      <td>query { hiring { employee_id shop_id is_full_time start_from } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>query { teacher ( order_by : { age : asc } ) { name } }</td>\n",
       "      <td>query { teacher ( order_by : { age : asc } ) { name } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>query { pets ( where : { pet_age : { _gt : 1 } } ) { petid weight } }</td>\n",
       "      <td>query { pets ( where : { pet_age : { _gt : 1 } } ) { weight petid } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>query { singer_aggregate { aggregate { count } } }</td>\n",
       "      <td>query { singer_aggregate { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>query { singer { birth_year citizenship } }</td>\n",
       "      <td>query { singer { citizenship birth_year } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>query { ref_template_types { template_type_code template_type_description } }</td>\n",
       "      <td>query { ref_template_types { template_type_description template_type_code } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>query { airports ( where : { city : { _eq : \"Anthony\" } } ) { airportcode airportname }}</td>\n",
       "      <td>query { airports ( where : { city : { _eq : \"Anthony\" } } ) { airportname airportcode }}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>query { singer ( where : { song_name : { _like : \"%Hey%\" } } ) { name country } }</td>\n",
       "      <td>query { singer ( where : { song_name : { _like : \"%Hey%\" } } ) { country name } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>query { players ( order_by : { birth_date : asc } ) { first_name last_name } }</td>\n",
       "      <td>query { players ( order_by : { birth_date : asc } ) { last_name first_name } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>query { battle ( where : { bulgarian_commander : { _neq : \"Boril\" } } ) { name result } }</td>\n",
       "      <td>query { battle ( where : { bulgarian_commander : { _neq : \"Boril\" } } ) { result name } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>query { stadium ( where : { capacity : { _gte : 5000 , _lte : 10000 } } ) { location name } }</td>\n",
       "      <td>query { stadium ( where : { capacity : { _gte : 5000 , _lte : 10000 } } ) { name location } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>query { country_aggregate ( where : { continent : { _eq : \"Africa\" } } ) { aggregate { count } } }</td>\n",
       "      <td>query { country_aggregate ( where : { continent : { _eq : \"Africa\" } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }</td>\n",
       "      <td>query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }</td>\n",
       "      <td>query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>query { country_aggregate ( where : { governmentform : { _eq : \"Republic\" } } ) { aggregate { count } } }</td>\n",
       "      <td>query { country_aggregate ( where : { governmentform : { _eq : \"Republic\" } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>query { airlines_aggregate ( where : { _and : { flights : { sourceairport : { _eq : \"AHD\" } } , airline : { _eq : \"United Airlines\" } } } ) { aggregate { count } } }</td>\n",
       "      <td>query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { sourceairport : { _eq : \"AHD\" } } } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { destairport : { _eq : \"ASY\" } } } } ) { aggregate { count } } }</td>\n",
       "      <td>query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { destairport : { _eq : \"ASY\" } } } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>query { paragraphs_aggregate ( where : { document : { document_name : { _eq : \"Summer Show\" } } } ) { aggregate { count } } }</td>\n",
       "      <td>query { paragraphs_aggregate ( where : { document : { document_name : { _eq : \"Summer Show\" } } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>query { country_aggregate ( where : { _and : { countrylanguages : { isofficial : { _eq : \"T\" } } , name : { _eq : \"Afghanistan\" } } } ) { aggregate { count } } }</td>\n",
       "      <td>query { country_aggregate ( where : { _and : { name : { _eq : \"Afghanistan\" } , countrylanguages : { isofficial : { _eq : \"T\" } } } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>query { airlines_aggregate ( where : { _and : { flights : { sourceairport : { _eq : \"AHD\" } } , airline : { _eq : \"United Airlines\" } } } ) { aggregate { count } } }</td>\n",
       "      <td>query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { sourceairport : { _eq : \"AHD\" } } } } ) { aggregate { count } } }</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  queries                                                                                                                                                  queries_shifted_order  result\n",
       "0                                                                                                        query { hiring { employee_id is_full_time shop_id start_from } }                                                                                                       query { hiring { employee_id shop_id is_full_time start_from } }    True\n",
       "1                                                                                                                 query { teacher ( order_by : { age : asc } ) { name } }                                                                                                                query { teacher ( order_by : { age : asc } ) { name } }    True\n",
       "2                                                                                                   query { pets ( where : { pet_age : { _gt : 1 } } ) { petid weight } }                                                                                                  query { pets ( where : { pet_age : { _gt : 1 } } ) { weight petid } }    True\n",
       "3                                                                                                                      query { singer_aggregate { aggregate { count } } }                                                                                                                     query { singer_aggregate { aggregate { count } } }    True\n",
       "4                                                                                                                             query { singer { birth_year citizenship } }                                                                                                                            query { singer { citizenship birth_year } }    True\n",
       "5                                                                                           query { ref_template_types { template_type_code template_type_description } }                                                                                          query { ref_template_types { template_type_description template_type_code } }    True\n",
       "6                                                                                query { airports ( where : { city : { _eq : \"Anthony\" } } ) { airportcode airportname }}                                                                               query { airports ( where : { city : { _eq : \"Anthony\" } } ) { airportname airportcode }}    True\n",
       "7                                                                                       query { singer ( where : { song_name : { _like : \"%Hey%\" } } ) { name country } }                                                                                      query { singer ( where : { song_name : { _like : \"%Hey%\" } } ) { country name } }    True\n",
       "8                                                                                          query { players ( order_by : { birth_date : asc } ) { first_name last_name } }                                                                                         query { players ( order_by : { birth_date : asc } ) { last_name first_name } }    True\n",
       "9                                                                               query { battle ( where : { bulgarian_commander : { _neq : \"Boril\" } } ) { name result } }                                                                              query { battle ( where : { bulgarian_commander : { _neq : \"Boril\" } } ) { result name } }    True\n",
       "10                                                                          query { stadium ( where : { capacity : { _gte : 5000 , _lte : 10000 } } ) { location name } }                                                                          query { stadium ( where : { capacity : { _gte : 5000 , _lte : 10000 } } ) { name location } }    True\n",
       "11                                                                     query { country_aggregate ( where : { continent : { _eq : \"Africa\" } } ) { aggregate { count } } }                                                                     query { country_aggregate ( where : { continent : { _eq : \"Africa\" } } ) { aggregate { count } } }    True\n",
       "12                                                                         query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }                                                                         query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }    True\n",
       "13                                                                         query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }                                                                         query { airlines_aggregate ( where : { country : { _eq : \"USA\" } } ) { aggregate { count } } }    True\n",
       "14                                                              query { country_aggregate ( where : { governmentform : { _eq : \"Republic\" } } ) { aggregate { count } } }                                                              query { country_aggregate ( where : { governmentform : { _eq : \"Republic\" } } ) { aggregate { count } } }    True\n",
       "15  query { airlines_aggregate ( where : { _and : { flights : { sourceairport : { _eq : \"AHD\" } } , airline : { _eq : \"United Airlines\" } } } ) { aggregate { count } } }  query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { sourceairport : { _eq : \"AHD\" } } } } ) { aggregate { count } } }    True\n",
       "16    query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { destairport : { _eq : \"ASY\" } } } } ) { aggregate { count } } }    query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { destairport : { _eq : \"ASY\" } } } } ) { aggregate { count } } }    True\n",
       "17                                          query { paragraphs_aggregate ( where : { document : { document_name : { _eq : \"Summer Show\" } } } ) { aggregate { count } } }                                          query { paragraphs_aggregate ( where : { document : { document_name : { _eq : \"Summer Show\" } } } ) { aggregate { count } } }    True\n",
       "18      query { country_aggregate ( where : { _and : { countrylanguages : { isofficial : { _eq : \"T\" } } , name : { _eq : \"Afghanistan\" } } } ) { aggregate { count } } }      query { country_aggregate ( where : { _and : { name : { _eq : \"Afghanistan\" } , countrylanguages : { isofficial : { _eq : \"T\" } } } } ) { aggregate { count } } }    True\n",
       "19  query { airlines_aggregate ( where : { _and : { flights : { sourceairport : { _eq : \"AHD\" } } , airline : { _eq : \"United Airlines\" } } } ) { aggregate { count } } }  query { airlines_aggregate ( where : { _and : { airline : { _eq : \"United Airlines\" } , flights : { sourceairport : { _eq : \"AHD\" } } } } ) { aggregate { count } } }    True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to next line\n",
    "pd.set_option('display.max_colwidth', None)  # Show the full content of each cell\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Create a datafame called test_df from semantic_matching_queries.csv\n",
    "\n",
    "test_df = pd.read_csv('semantic_match_test_queries.csv')\n",
    "\n",
    "test_df.head()\n",
    "\n",
    "# for every row in the dataframe call are_queries_semantically_same with queries and queries_shifted_order. Create a new column called result and set it to the result of the function call\n",
    "\n",
    "test_df['result'] = test_df.apply(lambda row: are_queries_semantically_same(row['queries'], row['queries_shifted_order']), axis=1)\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def custom_test(dataset_path):\n",
    "    # Load the dev dataset\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dev_data = json.load(f)\n",
    "\n",
    "    exact_match = []\n",
    "    semantic_match = []\n",
    "\n",
    "    for example in dev_data:\n",
    "        prompt = example[\"question\"]\n",
    "        schemaId = example[\"schemaId\"]\n",
    "        query = example[\"query\"]\n",
    "\n",
    "        prediction = predict(prompt, schemaId)[\"prediction\"]\n",
    "\n",
    "        exact_match.append(1 if prediction == query else 0)\n",
    "        semantic_match.append(1 if are_queries_semantically_same(prediction, query) else 0)\n",
    "\n",
    "    def calculate_metrics(match_results):\n",
    "        accuracy = accuracy_score(match_results, np.ones(len(match_results)))\n",
    "        f1 = f1_score(match_results, np.ones(len(match_results)))\n",
    "        precision = precision_score(match_results, np.ones(len(match_results)))\n",
    "        recall = recall_score(match_results, np.ones(len(match_results)))\n",
    "        return accuracy, f1, precision, recall\n",
    "\n",
    "    # Calculate evaluation metrics for exact match\n",
    "    exact_accuracy, exact_f1, exact_precision, exact_recall = calculate_metrics(exact_match)\n",
    "\n",
    "    # Calculate evaluation metrics for semantic match\n",
    "    semantic_accuracy, semantic_f1, semantic_precision, semantic_recall = calculate_metrics(semantic_match)\n",
    "\n",
    "    print(\"Exact match:\")\n",
    "    print(f\"Accuracy: {exact_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nSemantic match:\")\n",
    "    print(f\"Accuracy: {semantic_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def custom_test_df(df):\n",
    "    exact_match = []\n",
    "    semantic_match = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row[\"question\"]\n",
    "        schemaId = row[\"schemaId\"]\n",
    "        query = row[\"query\"]\n",
    "\n",
    "        prediction = predict(prompt, schemaId)[\"prediction\"]\n",
    "\n",
    "        exact_match.append(1 if prediction == query else 0)\n",
    "        df.loc[index, 'exact_match'] = 1 if prediction == query else 0\n",
    "\n",
    "        semantic_match.append(1 if are_queries_semantically_same(prediction, query) else 0)\n",
    "        df.loc[index, 'semantic_match'] = 1 if are_queries_semantically_same(prediction, query) else 0\n",
    "\n",
    "        # Add predicted query to the dataframe\n",
    "        df.loc[index, 'predicted_query'] = prediction\n",
    "\n",
    "    def calculate_metrics(match_results):\n",
    "        accuracy = accuracy_score(match_results, np.ones(len(match_results)))\n",
    "        return accuracy\n",
    "\n",
    "    # Calculate evaluation metrics for exact match\n",
    "    exact_accuracy = calculate_metrics(exact_match)\n",
    "\n",
    "    # Calculate evaluation metrics for semantic match\n",
    "    semantic_accuracy = calculate_metrics(semantic_match)\n",
    "\n",
    "    return exact_accuracy, semantic_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.task = 'finetune'\n",
    "system.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the result\n",
      "{'prediction': 'query { ship_aggregate ( where : { tonnage : { _lt : \"Captured\" } } ) { aggregate { count } } }'}\n"
     ]
    }
   ],
   "source": [
    "hardcoded_schemaId = \"battle_death\"\n",
    "hardcoded_prompt = \"How many ships ended up being not 'Captured'?\"\n",
    "\n",
    "result = predict(hardcoded_prompt, hardcoded_schemaId)\n",
    "print(\"this is the result\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset_path = 'SPEGQL-dataset/dataset/dev.json'\n",
    "\n",
    "#custom_test(dev_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5555555555555556, 0.8888888888888888)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dataframe from dev_df.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dev_df = pd.read_csv('dev_df.csv')\n",
    "\n",
    "# Add two empty columns to the dataframe: exact match and semantic match\n",
    "\n",
    "dev_df['exact_match'] = \"\"\n",
    "dev_df['semantic_match'] = \"\"\n",
    "\n",
    "#custom_test_df(dev_df)\n",
    "\n",
    "# Save the dataframe to a csv file for later use. Call it dev_df__with_results.csv\n",
    "\n",
    "# dev_df.to_csv('dev_df_with_results.csv')\n",
    "\n",
    "# Create a dataframe from dev_df with 5% of the data\n",
    "\n",
    "dev_df_20 = dev_df.sample(frac=0.02)\n",
    "custom_test_df(dev_df_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>schemaId</th>\n",
       "      <th>question</th>\n",
       "      <th>question_length</th>\n",
       "      <th>question_length_bucket</th>\n",
       "      <th>nesting_level</th>\n",
       "      <th>num_args</th>\n",
       "      <th>schema_complexity</th>\n",
       "      <th>schema_total_complexity</th>\n",
       "      <th>schema_types_count</th>\n",
       "      <th>schema_fields_count</th>\n",
       "      <th>schema_input_objects_count</th>\n",
       "      <th>schema_relationships_count</th>\n",
       "      <th>schema_arguments_count</th>\n",
       "      <th>schema_length</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>semantic_match</th>\n",
       "      <th>predicted_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>query { singer ( where : { songs : { sales : { _gt : 300000.0 } } } , distinct_on : name ) { name } }</td>\n",
       "      <td>singer</td>\n",
       "      <td>what are the different names of the singers that have sales more than 300000?</td>\n",
       "      <td>77</td>\n",
       "      <td>Long</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6913.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>106333.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>query { singer ( where : { songs : { sales : { _gt : 300000 } } } , distinct_on : name ) { name } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>query { stadium_aggregate { aggregate { avg { capacity } max { capacity } } } }</td>\n",
       "      <td>concert_singer</td>\n",
       "      <td>What is the average and maximum capacities for all stations?</td>\n",
       "      <td>60</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12079.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>186103.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { stadium_aggregate { aggregate { avg { capacity } max { capacity } } } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>query { tv_series_aggregate { aggregate { max { share } min { share } } } }</td>\n",
       "      <td>tvshow</td>\n",
       "      <td>What is the maximum and minimum share for the TV series?</td>\n",
       "      <td>56</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9663.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>148873.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { tv_series_aggregate { aggregate { max { share } min { share } } } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>query { visitor_aggregate ( where : { age : { _lt : 30 } } ) { aggregate { count } } }</td>\n",
       "      <td>museum_visit</td>\n",
       "      <td>How many visitors below age 30 are there?</td>\n",
       "      <td>41</td>\n",
       "      <td>Short</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9140.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>140442.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { visitor_aggregate ( where : { age : { _lt : 30 } } ) { aggregate { count } } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>query { pets ( where : { pet_age : { _gt : 1 } } ) { petid weight } }</td>\n",
       "      <td>pets_1</td>\n",
       "      <td>What is the id and weight of every pet who is older than 1?</td>\n",
       "      <td>59</td>\n",
       "      <td>Medium</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9543.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>145292.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>query { pets ( where : { age : { _gt : 1 } } ) { petid weight } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>query { matches_aggregate { aggregate { avg { winner_rank } } } }</td>\n",
       "      <td>wta_1</td>\n",
       "      <td>What is the average rank for winners in all matches?</td>\n",
       "      <td>52</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14974.0</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>220246.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>query { matches_aggregate { aggregate { avg { rank } } } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>query { airports ( where : { city : { _eq : \"Anthony\" } } ) { airportcode airportname } }</td>\n",
       "      <td>flight_2</td>\n",
       "      <td>Give the airport code and airport name corresonding to the city Anthony.</td>\n",
       "      <td>72</td>\n",
       "      <td>Long</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8282.0</td>\n",
       "      <td>646.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>129636.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>query { airports ( where : { city : { _eq : \"Atlanta\" } } ) { airportcode airportname } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { content } }</td>\n",
       "      <td>tvshow</td>\n",
       "      <td>What is the content of TV Channel with serial name \"Sky Radio\"?</td>\n",
       "      <td>63</td>\n",
       "      <td>Long</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9663.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>148873.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { content } }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { package_option } }</td>\n",
       "      <td>tvshow</td>\n",
       "      <td>What are the Package Options of the TV Channels whose series names are Sky Radio?</td>\n",
       "      <td>81</td>\n",
       "      <td>Very Long</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9663.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>148873.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { package_option } }</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     query        schemaId                                                                           question  question_length question_length_bucket  nesting_level  num_args  schema_complexity  schema_total_complexity  schema_types_count  schema_fields_count  schema_input_objects_count  schema_relationships_count  schema_arguments_count  schema_length exact_match semantic_match                                                                                      predicted_query\n",
       "276  query { singer ( where : { songs : { sales : { _gt : 300000.0 } } } , distinct_on : name ) { name } }          singer      what are the different names of the singers that have sales more than 300000?               77                   Long              4         4             6913.0                    538.0                91.0                167.0                        41.0                       167.0                    72.0       106333.0           0              1  query { singer ( where : { songs : { sales : { _gt : 300000 } } } , distinct_on : name ) { name } }\n",
       "23                         query { stadium_aggregate { aggregate { avg { capacity } max { capacity } } } }  concert_singer                       What is the average and maximum capacities for all stations?               60                 Medium              4         0            12079.0                    958.0               163.0                282.0                        79.0                       282.0                   152.0       186103.0           1              1                      query { stadium_aggregate { aggregate { avg { capacity } max { capacity } } } }\n",
       "304                            query { tv_series_aggregate { aggregate { max { share } min { share } } } }          tvshow                           What is the maximum and minimum share for the TV series?               56                 Medium              4         0             9663.0                    739.0               112.0                223.0                        53.0                       223.0                   128.0       148873.0           1              1                          query { tv_series_aggregate { aggregate { max { share } min { share } } } }\n",
       "162                 query { visitor_aggregate ( where : { age : { _lt : 30 } } ) { aggregate { count } } }    museum_visit                                          How many visitors below age 30 are there?               41                  Short              3         3             9140.0                    729.0               127.0                215.0                        60.0                       215.0                   112.0       140442.0           1              1               query { visitor_aggregate ( where : { age : { _lt : 30 } } ) { aggregate { count } } }\n",
       "224                                  query { pets ( where : { pet_age : { _gt : 1 } } ) { petid weight } }          pets_1                        What is the id and weight of every pet who is older than 1?               59                 Medium              3         3             9543.0                    768.0               127.0                229.0                        61.0                       229.0                   122.0       145292.0           0              1                                    query { pets ( where : { age : { _gt : 1 } } ) { petid weight } }\n",
       "409                                      query { matches_aggregate { aggregate { avg { winner_rank } } } }           wta_1                               What is the average rank for winners in all matches?               52                 Medium              4         0            14974.0                   1147.0               125.0                419.0                        60.0                       419.0                   124.0       220246.0           0              0                                           query { matches_aggregate { aggregate { avg { rank } } } }\n",
       "125              query { airports ( where : { city : { _eq : \"Anthony\" } } ) { airportcode airportname } }        flight_2           Give the airport code and airport name corresonding to the city Anthony.               72                   Long              3         3             8282.0                    646.0               112.0                173.0                        52.0                       173.0                   136.0       129636.0           0              1            query { airports ( where : { city : { _eq : \"Atlanta\" } } ) { airportcode airportname } }\n",
       "289                   query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { content } }          tvshow                    What is the content of TV Channel with serial name \"Sky Radio\"?               63                   Long              3         3             9663.0                    739.0               112.0                223.0                        53.0                       223.0                   128.0       148873.0           1              1                 query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { content } }\n",
       "292            query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { package_option } }          tvshow  What are the Package Options of the TV Channels whose series names are Sky Radio?               81              Very Long              3         3             9663.0                    739.0               112.0                223.0                        53.0                       223.0                   128.0       148873.0           1              1          query { tv_channel ( where : { series_name : { _eq : \"Sky Radio\" } } ) { package_option } }"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent wrapping to next line\n",
    "pd.set_option('display.max_colwidth', None)  # Show the full content of each cell\n",
    "\n",
    "dev_df_20\n",
    "\n",
    "# How many rows are in the dataframe?\n",
    "\n",
    "# len(dev_df_20.index)\n",
    "\n",
    "# # How many exact matches are there?\n",
    "\n",
    "# dev_df_20['exact_match'].sum()\n",
    "\n",
    "# # How many semantic matches are there?\n",
    "\n",
    "# dev_df_20['semantic_match'].sum()\n",
    "\n",
    "# dev_df_20.to_csv('dev_df_20_with_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have this function that predicts the corresponding query to a question.\n",
    "# I pass this data frame to the function. The data frame consists of the following columns:\n",
    "\n",
    "# I wish to know how well it performs on the the different properties: question_length_bucket, nesting level, num_args, schema length, schema complexity\n",
    "# Additionally, I wish to know how well it performs on the interactions of these properties. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd51bf327c04802b512fe352f9afa71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/jakobtolstrup/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'TextToGraphQLDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# system.num_beams = 3\n",
    "# system.test_flag = 'graphql'\n",
    "# system.prepare_data()\n",
    "# trainer.test(system)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7048a5d75593413cc54dc24206831079ac8905ffddb319c4eafd454be0ec5d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
